{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "524d8a73-fa64-4da6-b05c-60ab54889fae",
   "metadata": {},
   "source": [
    "## <span style=\"color:#90BE6D\"><b> Dataset Description:</span> \n",
    "\n",
    "This dataset provides comprehensive information about customers of a bank (Universal Bank), designed to analyze their financial behavior and response to a personal loan offer. The data includes not only demographic characteristics of the customers but also their banking behavior and level of engagement with the bank's financial products.\n",
    "\n",
    "ID: Customer ID\n",
    "\n",
    "Age: Customer's age in completed years\n",
    "\n",
    "Experience: Years of work experience\n",
    "\n",
    "Income: Annual income (in thousands of dollars)\n",
    "\n",
    "Zipcode: ZIP code of the customer’s residence\n",
    "\n",
    "Family: Number of family members\n",
    "\n",
    "CCAvg: Average monthly spending using the credit card (in thousands of dollars)\n",
    "\n",
    "Education: Education level (1: Bachelor's, 2: Master's, 3: Advanced/Professional degree)\n",
    "\n",
    "Mortgage: Value of home mortgage (if any), in thousands of dollars\n",
    "\n",
    "Securities Account: Does the customer have a securities account with the bank?\n",
    "\n",
    "CD Account: Does the customer have a certificate of deposit (CD) account with the bank?\n",
    "\n",
    "Online: Does the customer use internet banking services?\n",
    "\n",
    "CreditCard: Does the customer use a credit card issued by the bank?\n",
    "\n",
    "Personal Loan: Did the customer accept the personal loan offered in the previous campaign? (Target variable)\n",
    "\n",
    "## <span style=\"color:#90BE6D\"><b> Analysis Objective:</span> \n",
    "\n",
    "The objective of this project is not merely to predict personal loan acceptance. Rather, the aim is to uncover hidden patterns in customer financial behavior and use them to build a model that can estimate the likelihood of loan acceptance. Beyond prediction, the goal is to assist the marketing team in crafting more accurate, targeted, and personalized financial offers. This analysis can contribute to improving the conversion rates of future campaigns and identifying high-value customers.\n",
    "\n",
    "In this notebook, we use logistic regression, KNN and Navie Bayes classification models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b2df19-1ac3-4f8d-b256-9d48b86c0270",
   "metadata": {},
   "source": [
    "## <b> Importting Libraries and Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce45868d-3291-43a4-b71c-168315dc0d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data manipulation ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- Visualization libraries ---\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "\n",
    "# --- Model selection & evaluation tools ---\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold, RandomizedSearchCV\n",
    "\n",
    "# --- Models ---\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# --- Preprocessing & transformation ---\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler, OneHotEncoder, FunctionTransformer\n",
    "\n",
    "# --- Evaluation metrics ---\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, ConfusionMatrixDisplay,\n",
    "    roc_auc_score, roc_curve, auc, accuracy_score, precision_score, recall_score, f1_score,\n",
    "    precision_recall_curve, average_precision_score\n",
    ")  \n",
    "\n",
    "# --- Model interpretation ---\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# --- Geospatial visualization ---\n",
    "import folium\n",
    "from branca.element import Element\n",
    "\n",
    "# --- IPython display tools ---\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# --- Misc ---\n",
    "from scipy.stats import randint\n",
    "\n",
    "# --- Warning configuration ---\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa03e2b-90a0-4e04-92d5-418c4fb1987e",
   "metadata": {},
   "source": [
    "## <b> Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85dda8b4-ab99-46ad-b69b-a5210655c0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from a CSV file\n",
    "data = pd.read_csv('Bank_Personal_Loan_Modelling.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a434c697-f3fa-4fa6-8209-73df22b3b33d",
   "metadata": {},
   "source": [
    "## <b> Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dcd77f-91e9-4b7a-89c5-32a47843571e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Dataframe of dataset\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a37843-155b-48c7-ac6f-c95f53598285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display information about the DataFrame\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5734da-5af7-4426-8a2c-e30b1f9255e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a statistical summary of numerical columns in the DataFrame\n",
    "df.describe(include='number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf094af1-9ead-4c6e-843c-686387c4978c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a statistical summary of categorical (non-numeric) columns in the DataFrame\n",
    "df.describe(include='object')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c73ac9-4df9-4270-8aac-4404ebe5563e",
   "metadata": {},
   "source": [
    "### <b> Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa37a661-ebb6-43c7-b4d6-35ba4180a6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count NaN values\n",
    "missing_values = df.isnull().sum()\n",
    "missing_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98be96c3-5e2e-410e-b78b-9a408e24da86",
   "metadata": {},
   "source": [
    "<b><b> conclusion\n",
    "\n",
    "🔶 The dataset contains 5,000 rows and 14 columns, with \"Personal Loan\" as the target variable.\n",
    "\n",
    "🔶 Out of the 14 columns, 13 are numerical and 1 column (\"CCAvg\") is of object type.\n",
    "\n",
    "🔶 The columns <b>\"ID\", \"Age\", \"Experience\", \"Income\", \"CC_Avg\", and \"Mortgage\"</b> are numerical, while the columns <b>\"Family\", \"Education\", \"Zip_Code\", Personal_Loan , Securities Account , CD_Account , Online , Credit_Card</b> are categorical numerical columns.\n",
    "\n",
    "🔶 The column <b>\"CCAvg\"</b> is a decimal data, so we need to change the \"/\" to \".\" and change the data type to float.\n",
    "\n",
    "🔶 The \"Experience\" column contains negative values, which are incorrect and need to be corrected or removed.\n",
    "\n",
    "🔶 There are no missing values in the data.\n",
    "\n",
    "🔶 The \"ID\" and \"ZipCode\" columns do not have any impact on the target variable, which is \"Personal Loan\", so we will remove them.\n",
    "\n",
    "🔶 CCAVG represents the average monthly credit card spending, but Income represents the annual income. To keep the units of the attributes the same, we need to convert the average monthly credit card spending to an annualized amount."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff13dd13-5b4e-48a9-8e1a-b6d69e6c16c2",
   "metadata": {},
   "source": [
    "### <b> Change in columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145220ba-3445-489b-96aa-ebf785b587a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A deep copy of the DataFrame is created to apply modifications to it.\n",
    "df1 = df.copy(deep = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bff39bb-eb83-41ba-bf0f-4765938ee2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['CCAvg'] = df1['CCAvg'].str.replace('/', '.').astype(float)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6376c2f9-5129-45aa-a065-f359e3c06303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a statistical summary of all columns, including both numeric and categorical\n",
    "df1.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ab0854-a1fe-4e10-a629-cdb40abcef4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count of negative values in the 'Experience' column by each distinct value\n",
    "negative_counts = df1[df1['Experience'] < 0]['Experience'].value_counts()\n",
    "\n",
    "print(negative_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb80c024-7194-448d-95fc-44c277806ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert negative values in the 'Experience' column to positive (absolute value)\n",
    "df1['Experience'] = df1['Experience'].abs()\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057a68e4-184e-41ed-bdfe-ef173b1d92a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting the number of duplicate rows\n",
    "num_duplicates = df1.duplicated().sum()\n",
    "\n",
    "print(f\" The number of duplicate rows in the DataFrame.: {num_duplicates}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755f5341-982b-4b30-9f9b-f2a35f2e8dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 'ID'column from dataframe\n",
    "df1 = df1.drop(['ID'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cad42a-6803-4588-9cc1-74c77ec815ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the \"CCAvg\" unit from monthly to yearly.\n",
    "df1['CCAvg'] *= 12\n",
    "# Rename the 'CCAvg' column to 'CCAvg_Annual'\n",
    "df1.rename(columns={'CCAvg': 'CCAvg_Annual'}, inplace=True)\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67210a8-af86-4753-911e-74f72d3258f8",
   "metadata": {},
   "source": [
    "### <b> Noise Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7939dd87-9470-43b6-a9ec-c1d82e0d2582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the visual style for the plots using seaborn\n",
    "sns.set(style=\"whitegrid\", palette=\"pastel\")\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Plot a histogram for ZIP Code distribution\n",
    "hist_data = sns.histplot(data=df1, x='ZIP Code', bins=50, kde=False, color='#90BE6D', edgecolor='black')\n",
    "\n",
    "# Add value labels on top of each bar\n",
    "for patch in hist_data.patches:\n",
    "    height = patch.get_height()\n",
    "    if height > 0:\n",
    "        plt.text(patch.get_x() + patch.get_width() / 2, height + 1,\n",
    "                 int(height), ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "# Identify ZIP Codes that are potentially invalid\n",
    "highlighted = df1[df1['ZIP Code'] < 20000]\n",
    "if not highlighted.empty:\n",
    "    zip_val = highlighted['ZIP Code'].values[0]\n",
    "\n",
    "    # Highlight the unusual ZIP Code with a circle\n",
    "    plt.scatter([zip_val], [5], s=600, facecolors='none', edgecolors='#EA9010', linewidths=2, label=f'Noisy data: {zip_val}')\n",
    "\n",
    "    # Add the legend to explain the annotation\n",
    "    plt.legend(loc='upper left', fontsize=11)\n",
    "\n",
    "# Add title and labels to the plot\n",
    "plt.title('Distribution of ZIP Codes with Annotation', fontsize=12, fontweight='bold')\n",
    "plt.xlabel('ZIP Code', fontsize=10)\n",
    "plt.ylabel('Number of Customers', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2aba59-bf55-4841-83c8-2dc2e5e43ef2",
   "metadata": {},
   "source": [
    "🔶 The ZIP Code column has a minimum value that is a significant outlier compared to the mean. \n",
    "\n",
    "🔶 After examining the distribution of this column, we confirmed that it is indeed an outlier, so we will remove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e738136-aabe-4954-8a80-861912cb9c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows where the 'ZIP Code' is less than 20000\n",
    "df1.drop(df1[df1['ZIP Code']<20000].index, inplace=True)\n",
    "# Reset the index of the DataFrame after removing the rows, and drop the old index column\n",
    "df1.reset_index(drop=True, inplace =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31d10f4-eabf-4c98-a103-d862837694d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set higher resolution for the plots\n",
    "mpl.rcParams['figure.dpi'] = 200  # Higher resolutionا\n",
    "\n",
    "# List of selected features to plot\n",
    "selected_features = [\"Age\", \"Experience\", \"Income\", \"CCAvg_Annual\", \"Mortgage\"]\n",
    "n = len(selected_features)\n",
    "\n",
    "# Create subplots with one column for boxplots and one column for histograms + KDE\n",
    "fig, axes = plt.subplots(n, 2, figsize=(12, n * 3))\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "for i, col in enumerate(selected_features):\n",
    "    # Box plot\n",
    "    sns.boxplot(\n",
    "        data=df1,\n",
    "        x=col,\n",
    "        ax=axes[i, 0],\n",
    "        color='#90BE6D',\n",
    "        flierprops=dict(marker='o', markerfacecolor='#EA9010', markeredgecolor='#EA9010', markersize=5)\n",
    "    )\n",
    "    axes[i, 0].tick_params(axis='x', labelrotation=90)\n",
    "    axes[i, 0].set_xlabel(col, fontweight='bold')\n",
    "    axes[i, 0].grid(True)  # Enable grid\n",
    "\n",
    "    # Histogram + KDE\n",
    "    sns.histplot(\n",
    "        df1[col],\n",
    "        ax=axes[i, 1],\n",
    "        color='#90BE6D',\n",
    "        stat='density', # Normalize the histogram\n",
    "        bins=30,\n",
    "        edgecolor='black'\n",
    "    )\n",
    "    sns.kdeplot(\n",
    "        df1[col],\n",
    "        ax=axes[i, 1],\n",
    "        color='#EA9010',\n",
    "        linewidth=2 # Set line width for KDE\n",
    "    )\n",
    "    axes[i, 1].tick_params(axis='x', labelrotation=90)\n",
    "    axes[i, 1].set_xlabel(col, fontweight='bold')\n",
    "    axes[i, 1].grid(True)\n",
    "\n",
    "# Set a title for the entire figure\n",
    "fig.suptitle('Distribution Plots for Numerical Features', fontsize=18, fontweight='bold')\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.98])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e889aed1-8c46-4cc9-a914-d2811a7c03a9",
   "metadata": {},
   "source": [
    "🔶 Age\n",
    "\n",
    "The average age of customers is around 45 years.\n",
    "\n",
    "The age distribution spans between approximately 25 and 70 years and appears to follow a uniform distribution. The boxplot shows that the data is nearly symmetrical, with no significant outliers.\n",
    "\n",
    "\n",
    "🔶 Experience\n",
    "\n",
    "The average work experience of customers is about 20 years.\n",
    "\n",
    "Customer work experience data is symmetrically distributed with a slight right skew.\n",
    "\n",
    "The distribution is similar to that of age, but its range is limited between 0 and about 45 years. The absence of negative or outlier values indicates proper data cleaning.\n",
    "\n",
    "\n",
    "🔶 Income\n",
    "\n",
    "The average income of customers is approximately $60,000 per year.\n",
    "\n",
    "Customer income ranges from $10,000 to $180,000 per year.\n",
    "\n",
    "Most of the data is concentrated between 0 and 100k.\n",
    "\n",
    "The income distribution is highly right-skewed, meaning that most people earn below $100K, and a few individuals with very high incomes create a long tail (indicating many outliers at the upper end).\n",
    "\n",
    "This distribution is not normal and follows a power law (Pareto Principle – 80/20). Outliers in the data can introduce bias in models sensitive to distance, such as KNN or regression.\n",
    "\n",
    "\n",
    "🔶 CCAvg\n",
    "\n",
    "The CCAvg Annual is about $18,000.\n",
    "\n",
    "CCAvg Annual has a distribution with a strong left skew and many outliers.\n",
    "\n",
    "Similar to income, the CCAvg Annual distribution is highly skewed. Many outliers above 30 are observed, with most customers having lower values.\n",
    "\n",
    "\n",
    "🔶 Mortgage\n",
    "\n",
    "Mortgage data has a median of around 0.\n",
    "\n",
    "It has a distribution with a very strong left skew and many outliers.\n",
    "\n",
    "The distribution is highly imbalanced; most values are close to zero, while a few values are very high (up to more than 600). The boxplot indicates a significant number of outliers.\n",
    "\n",
    "🔶 Therefore, we use the IQR (Interquartile Range) method to identify and remove outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63230913-c22b-494d-a1b4-6f2aef09827b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 & 2: Calculate Q1, Q3, and IQR\n",
    "Q1 = df1['Mortgage'].quantile(0.25)\n",
    "Q3 = df1['Mortgage'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Step 3: Calculate lower and upper bounds\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Step 4: Filter out outliers to keep only non-outlier values\n",
    "df1_cleaned = df1[(df1['Mortgage'] >= lower_bound) & (df1['Mortgage'] <= upper_bound)]\n",
    "\n",
    "# Optional: Print the number of removed outliers\n",
    "print(f\"Number of removed outliers: {len(df1) - len(df1_cleaned)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6e170f-512f-4825-854a-ef6d0f363e20",
   "metadata": {},
   "source": [
    "🔶 Data outside the lower_bound and upper_bound are considered outliers.\n",
    "\n",
    "🔶 The IQR (Interquartile Range) method is one of the most common techniques for detecting outliers and has several important advantages over many other methods:\n",
    "\n",
    "🔶 1. Robustness to outliers\n",
    "\n",
    "Unlike the mean and standard deviation, which are themselves influenced by outliers, quartiles (Q1 and Q3) are not sensitive to them. This means:\n",
    "\n",
    "IQR maintains stable performance even in the presence of outliers.\n",
    "\n",
    "🔶 2. No distributional assumptions\n",
    "\n",
    "The IQR method does not assume that the data follows any specific distribution (e.g., normal distribution). Therefore:\n",
    "\n",
    "It works well for data that is skewed, imbalanced, or multimodal.\n",
    "\n",
    "🔶 3. Simple and interpretable\n",
    "\n",
    "IQR is calculated using only the quartiles, and its logic is easy to understand even for non-experts. Simply put:\n",
    "\n",
    "Anything that deviates too far from the “typical” range of the data is considered an outlier.\n",
    "\n",
    "🔶 4. Suitable for positive-only data (e.g., income, prices, loans)\n",
    "\n",
    "Some methods (like log transformation or Z-score) struggle with negative or zero values. However:\n",
    "\n",
    "IQR can be applied directly to such data without requiring prior transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511a7ad5-0db9-4386-9a79-4375bd0427fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set higher resolution for the plots\n",
    "mpl.rcParams['figure.dpi'] = 200  # Higher clarity.\n",
    "\n",
    "# List of selected categorical features to plot\n",
    "selected_features = [\"Family\", \"Education\", \"Personal Loan\", \"Securities Account\", \"CD Account\", \"Online\", \"CreditCard\"]\n",
    "\n",
    "n = len(selected_features)\n",
    "\n",
    "# create figure\n",
    "fig, axes = plt.subplots(n, 2, figsize=(12, n * 3))\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "for i, col in enumerate(selected_features):\n",
    "    # Count plot\n",
    "    sns.countplot(\n",
    "        data=df1_cleaned,\n",
    "        x=col,\n",
    "        ax=axes[i, 0],\n",
    "        palette=['#90BE6D'],\n",
    "        edgecolor='black'\n",
    "    )\n",
    "    axes[i, 0].tick_params(axis='x', labelrotation=0)\n",
    "    axes[i, 0].set_xlabel(col, fontweight='bold')\n",
    "    axes[i, 0].set_ylabel(\"Count\", fontweight='bold')\n",
    "\n",
    "    # Histogram + KDE\n",
    "    sns.histplot(\n",
    "        df1_cleaned[col],\n",
    "        ax=axes[i, 1],\n",
    "        color='#90BE6D',\n",
    "        stat='density', # Normalize the histogram to show the density\n",
    "        bins=30,\n",
    "        edgecolor='black'\n",
    "    )\n",
    "    sns.kdeplot(\n",
    "        df1_cleaned[col],\n",
    "        ax=axes[i, 1],\n",
    "        color='#EA9010',\n",
    "        linewidth=2 # Set line width for the KDE plot\n",
    "    )\n",
    "    axes[i, 1].tick_params(axis='x', labelrotation=0)\n",
    "    axes[i, 1].set_xlabel(col, fontweight='bold')\n",
    "\n",
    "# Set a main title for the entire figure\n",
    "fig.suptitle('Distribution Plots for Categorical Features', fontsize=18, fontweight='bold')\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.98])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdba49c2-8fb8-48e8-acd9-bfbc1ca89749",
   "metadata": {},
   "source": [
    "🔶 Family\n",
    "\n",
    "The distribution of \"Family\" is relatively uniform, meaning that customers are almost equally distributed in the 1 to 4 member groups. This indicates that this feature lacks severe bias towards a particular value and has a reasonable spread. It is likely informative on its own and does not require special processing.\n",
    "\n",
    "🔶 Education\n",
    "\n",
    "The values of \"Education\" only include 1, 2, and 3, representing an ordinal classification, probably indicating \"Bachelor's\", \"Master's\", and \"Doctorate\". The distribution is fairly normal, but group 1 has the highest frequency. This suggests that most customers have an average level of education.\n",
    "\n",
    "🔶 Personal Loan\n",
    "\n",
    "This is a binary variable (0 or 1), and its distribution is highly imbalanced. A very large number of people do not have a personal loan (majority class = 0), while only a small percentage (minority class) have a loan. This is an indication of class imbalance that should be addressed using techniques like oversampling or weight adjustment.\n",
    "\n",
    "🔶 Securities Account\n",
    "\n",
    "In this variable, the vast majority of users do not have a securities account. Similar to \"Personal Loan\", this represents a rare binary feature.\n",
    "\n",
    "🔶 CD Account\n",
    "\n",
    "This variable also has a highly imbalanced distribution, meaning only a small number of customers have a CD account.\n",
    "\n",
    "🔶 Online\n",
    "\n",
    "The distribution is relatively balanced, with almost half of the customers using online services.\n",
    "\n",
    "🔶 CreditCard\n",
    "\n",
    "In this variable, a large percentage of customers do not have a credit card, while a smaller portion have one.\n",
    "\n",
    "There aren't any noisy data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4774b49f-dfca-4e74-b1f4-f9162e926f9f",
   "metadata": {},
   "source": [
    "## <b> Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88dc362a-fd10-49fe-af90-d498a079ce15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new dataframe of selected columns\n",
    "selected_columns = df1_cleaned[[\"Age\", \"Experience\", \"Income\", \"Family\", \"CCAvg_Annual\", \"Education\", \"Mortgage\", \"Securities Account\", \"CD Account\", \"Online\", \"CreditCard\", \"Personal Loan\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed58ad8-3785-4cee-ae58-db8888104703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation\n",
    "Corrmat = selected_columns.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3d2a63-1d16-4414-b2a7-ae344dc229f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting correlation with heatmap\n",
    "plt.figure(figsize = (10, 5), dpi = 200)\n",
    "plt.title('Heatmap of Correlation', size=18, weight='bold')\n",
    "# Define a custom color palette for the heatmap\n",
    "custom_cmap = sns.color_palette([\"#90BE6D\", \"#C9E3AC\", \"#EA9010\"], as_cmap=True)\n",
    "sns.heatmap(Corrmat, annot=True, fmt=\".2f\", linewidth=0.5, cmap=custom_cmap, mask = np.triu(np.ones_like(Corrmat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0537c4be-7afe-4728-8bb8-c224efc14fe0",
   "metadata": {},
   "source": [
    "🔶 The target variable shows a significant correlation with the variables \"Income\", \"CCAvg_Annual\", and \"CD Account\".\n",
    "\n",
    "🔶 There is a very strong correlation between \"Age\" and \"Experience\" (0.99), as well as between \"Income\" and \"CCAvg_Annual\" (0.65)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e933f89f-05d6-4dab-8895-209980882bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the style for seaborn plots\n",
    "sns.set_style(\"whitegrid\")   \n",
    "\n",
    "# Set figure size and resolution\n",
    "plt.figure(figsize=(7, 3), dpi=150)\n",
    "\n",
    "# Create the count plot for 'Personal Loan' with hue\n",
    "ax = sns.countplot(\n",
    "    data=df1_cleaned,\n",
    "    x='Personal Loan',\n",
    "    hue='Personal Loan',\n",
    "    palette={0: '#90BE6D', 1: '#EA9010'} # Custom colors for 'No' and 'Yes'\n",
    ")\n",
    "\n",
    "# Title and labels\n",
    "plt.title('Count of Personal Loan Applicants', fontsize=12, fontweight='bold')\n",
    "plt.xlabel('Personal Loan (0 = No, 1 = Yes)', fontsize=10)\n",
    "plt.ylabel('Count', fontsize=9)\n",
    "\n",
    "# Remove legend\n",
    "ax.legend_.remove()\n",
    "\n",
    "# Customize grid\n",
    "ax.grid(True, axis='y', linewidth=1.2, color='lightgray')\n",
    "\n",
    "# Set color and thickness of spines\n",
    "for spine in ['top', 'right', 'left', 'bottom']:\n",
    "    ax.spines[spine].set_color('lightgray')\n",
    "    ax.spines[spine].set_linewidth(1.2)\n",
    "\n",
    "# Add percentage annotations\n",
    "total = df1_cleaned['Personal Loan'].value_counts().sum()\n",
    "for p in ax.patches:\n",
    "    count = p.get_height()\n",
    "    percent = (count / total) * 100\n",
    "    x = p.get_x() + p.get_width() / 2\n",
    "    y = count\n",
    "    if count > 0:\n",
    "        ax.text(x, y + 5, f'{percent:.1f}%', ha='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a8d1c4-eb1e-411d-aac7-60f148d35e0f",
   "metadata": {},
   "source": [
    "🔶 Only about 500 people (9.6%) in this dataset have received a loan, and around 4500 people (90.4%) have not received a loan.\n",
    "\n",
    "🔶 Therefore, it can be said that the data is <b>imbalanced. but small amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0f7a79-5cde-4015-8c87-21b9fbc43c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the appearance for the plots\n",
    "sns.set_theme(style=\"ticks\")\n",
    "\n",
    "# Create a pairplot to show relationships between selected numerical features and 'Personal Loan' status\n",
    "pair_plot = sns.pairplot(\n",
    "    df1_cleaned,  \n",
    "    hue=\"Personal Loan\",\n",
    "    vars=[\"Age\", \"Experience\", \"Income\", \"CCAvg_Annual\", \"Mortgage\"],  \n",
    "    corner=True, \n",
    "    palette={0: \"#90BE6D\", 1: \"#EA9010\"} # Custom color palette for 'No' and 'Yes' categories of 'Personal Loan'\n",
    ")\n",
    "\n",
    "# Set the main title for the plot\n",
    "plt.suptitle(\"Relationship between Numerical Features and Personal Loan Status\", fontsize=18, fontweight='bold')\n",
    "\n",
    "# Customize grid\n",
    "for ax in plt.gcf().axes:\n",
    "    ax.grid(True, which='both', axis='both', linestyle='--', linewidth=0.7, color='lightgray')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beda294e-6b23-4a6d-ae8d-b26942de0d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the appearance for the plots\n",
    "sns.set_theme(style=\"ticks\")\n",
    "\n",
    "# Create a pairplot to show relationships between selected categorical features and 'Personal Loan' status\n",
    "pair_plot = sns.pairplot(\n",
    "    df1_cleaned,  \n",
    "    hue=\"Personal Loan\",  \n",
    "    vars=[\"Family\", \"Education\", \"Personal Loan\", \"Securities Account\", \"CD Account\", \"Online\", \"CreditCard\"],  \n",
    "    corner=True, \n",
    "    palette={0: \"#90BE6D\", 1: \"#EA9010\"}\n",
    ")\n",
    "\n",
    "# Set the main title for the plot\n",
    "plt.suptitle(\"Relationship between Categorical Features and Personal Loan Status\", fontsize=18, fontweight='bold')\n",
    "\n",
    "# Customize grid\n",
    "for ax in plt.gcf().axes:\n",
    "    ax.grid(True, which='both', axis='both', linestyle='--', linewidth=0.7, color='lightgray')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cfb15e-1d0f-477a-aa97-c358c5bb0dea",
   "metadata": {},
   "source": [
    "🔶 In this project, we use the ZIP Code data for visualizing geographic locations and analyzing data points. After extracting the necessary information, we remove the ZIP Code column from the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de21994-33a6-4058-9f3f-e6c3173679c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of unique ZIP Codes present in the dataset\n",
    "unique_zips = df1_cleaned['ZIP Code'].unique()\n",
    "\n",
    "# Reference dataset\n",
    "# can download this file from public sources like: https://simplemaps.com/data/us-zips\n",
    "zip_df = pd.read_csv('uszips.csv')\n",
    "\n",
    "# Ensuring ZIP Code is numeric\n",
    "zip_df['zip'] = zip_df['zip'].astype(int)\n",
    "\n",
    "# Filter only the ZIP Codes that are present in the main dataset\n",
    "zip_map_df = zip_df[zip_df['zip'].isin(unique_zips)]\n",
    "\n",
    "# Merge the latitude, longitude, city, state_name, and county_name information with the original dataframe\n",
    "merged_df = df1_cleaned.merge(zip_map_df[['zip', 'lat', 'lng']], \n",
    "                              left_on='ZIP Code', right_on='zip', how='left')\n",
    "\n",
    "# Drop the redundant 'zip' column after the merge\n",
    "merged_df.drop(columns='zip', inplace=True)\n",
    "\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5aa0c87-f2b1-4e6c-9134-fe075e75bfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count NaN values\n",
    "missing_values2 = merged_df.isnull().sum()\n",
    "missing_values2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22352e13-3352-4aa3-8deb-8163028fd241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify missing ZIP Codes in df1_cleaned that are not in zip_df\n",
    "missing_zips = df1_cleaned[~df1_cleaned['ZIP Code'].isin(zip_df['zip'])]\n",
    "print(missing_zips['ZIP Code'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441999f0-3311-4842-a7ba-b36f2aab5262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a map from ZIP Code to complete geographic information\n",
    "zip_info_map = merged_df.dropna(subset=['lat', 'lng']).drop_duplicates(subset=['ZIP Code'])[\n",
    "    ['ZIP Code', 'lat', 'lng']\n",
    "].set_index('ZIP Code')\n",
    "\n",
    "# Function to fill missing geographic info row by row\n",
    "def fill_geo_info(row):\n",
    "    zip_code = row['ZIP Code']\n",
    "    if pd.isna(row['lat']) or pd.isna(row['lng']):    # If lat or lng are missing, try to fill them based on the ZIP Code\n",
    "        if zip_code in zip_info_map.index: \n",
    "            row['lat'] = zip_info_map.loc[zip_code, 'lat']\n",
    "            row['lng'] = zip_info_map.loc[zip_code, 'lng']\n",
    "    return row\n",
    "\n",
    "# Step 1: Fill missing geographic info based on available data\n",
    "merged_df = merged_df.apply(fill_geo_info, axis=1)\n",
    "\n",
    "# Step 2: Fill remaining missing values with default values\n",
    "mean_lat = merged_df['lat'].mean()\n",
    "mean_lng = merged_df['lng'].mean()\n",
    "\n",
    "# Filling remaining missing values with defaults\n",
    "merged_df['lat'].fillna(mean_lat, inplace=True)\n",
    "merged_df['lng'].fillna(mean_lng, inplace=True)\n",
    "\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a781253-5077-47a6-a72a-dab474c55a2e",
   "metadata": {},
   "source": [
    "🔶 We filled the missing values in the 'lat' and 'lng' columns with the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc789d29-65f5-48b9-ac05-4afd80d98edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count NaN values\n",
    "missing_values3 = merged_df.isnull().sum()\n",
    "missing_values3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595c940c-a14d-4412-80fc-8da5a8ab641c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the 'ZIP Code' column from the DataFrame\n",
    "merged_df.drop(columns='ZIP Code', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1705383-9879-44a1-9721-1e373cb3c4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the bounds of the map based on the latitude and longitude\n",
    "bounds = [[merged_df['lat'].min(), merged_df['lng'].min()],\n",
    "          [merged_df['lat'].max(), merged_df['lng'].max()]]\n",
    "\n",
    "# Create the base map\n",
    "m = folium.Map(zoom_start=6)\n",
    "m.fit_bounds(bounds)\n",
    "\n",
    "# Function to generate an icon based on the Personal Loan status\n",
    "def get_icon(personal_loan):\n",
    "    if personal_loan == 1:\n",
    "        return folium.Icon(color='#EA9010', icon='star', prefix='fa') # Icon for customers with a loan (orange color)\n",
    "    else:\n",
    "        return folium.Icon(color='#90BE6D', icon='star', prefix='fa') # Icon for customers without a loan (green color)\n",
    "\n",
    "# Add markers to the map\n",
    "for _, row in merged_df.iterrows():\n",
    "    popup_text = (\n",
    "        f\"<b>ZIP:</b> {row['ZIP Code']}<br>\"\n",
    "        f\"<b>Income:</b> ${row['Income']}<br>\"\n",
    "        f\"<b>Loan Status:</b> {'Yes' if row['Personal Loan'] == 1 else 'No'}\"\n",
    "    )\n",
    "    \n",
    "    folium.CircleMarker(\n",
    "    location=[row['lat'], row['lng']],\n",
    "    radius=10, \n",
    "    color='#EA9010' if row['Personal Loan'] == 1 else '#90BE6D',\n",
    "    opacity=0.7 if row['Personal Loan'] == 0 else 1.0,          # stroke opacity\n",
    "    fill=True,\n",
    "    fill_color='#EA9010' if row['Personal Loan'] == 1 else '#90BE6D',\n",
    "    fill_opacity=0.7 if row['Personal Loan'] == 0 else 0.8,\n",
    "    popup=popup_text\n",
    "    ).add_to(m)\n",
    "\n",
    "# Legend for Personal Loan status with clear positioning and size adjustments\n",
    "legend_html = \"\"\"\n",
    "<div style=\"\n",
    "    position: fixed;\n",
    "    bottom: 40px;\n",
    "    left: 40px;\n",
    "    width: 200px;\n",
    "    height: 100px;\n",
    "    background-color: white;\n",
    "    border: 2px solid #ccc;\n",
    "    border-radius: 8px;\n",
    "    z-index: 9999;\n",
    "    font-size: 14px;\n",
    "    font-family: Arial, sans-serif;\n",
    "    box-shadow: 2px 2px 6px rgba(0,0,0,0.3);\n",
    "    padding: 10px;\n",
    "\">\n",
    "<b style=\"font-size:16px;\">Personal Loan</b><br>\n",
    "<i class=\"fa fa-circle fa-lg\" style=\"color:#EA9010\"></i> Yes<br>\n",
    "<i class=\"fa fa-circle fa-lg\" style=\"color:#90BE6D\"></i> No\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "# Add the legend to the map\n",
    "m.get_root().html.add_child(Element(legend_html))\n",
    "\n",
    "display(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2899ba0-1fac-40c1-bda3-0c18b7033abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of unique ZIP Codes present in the dataset\n",
    "unique_zips = df1_cleaned['ZIP Code'].unique()\n",
    "\n",
    "# Reference dataset\n",
    "# can download this file from public sources like: https://simplemaps.com/data/us-zips\n",
    "zip_df = pd.read_csv('uszips.csv')\n",
    "\n",
    "# Ensuring ZIP Code is numeric\n",
    "zip_df['zip'] = zip_df['zip'].astype(int)\n",
    "\n",
    "# Filter only the ZIP Codes that are present in the main dataset\n",
    "zip_map_df = zip_df[zip_df['zip'].isin(unique_zips)]\n",
    "\n",
    "merged_df2 = merged_df.merge(zip_map_df[['zip', 'city']], \n",
    "                              left_on='ZIP Code', right_on='zip', how='left')\n",
    "merged_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb45b99-26b2-4ae6-94b1-6fcbd8688f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom colors for the 'Personal Loan' values\n",
    "custom_palette = {\n",
    "    1: '#EA9010',      # Customers with a loan\n",
    "    0: '#90BE6D'       # Customers without a loan\n",
    "}\n",
    "\n",
    "# Get the 10 most frequent cities\n",
    "top_cities = merged_df2['city'].value_counts().nlargest(20).index\n",
    "\n",
    "# Filter data to include only those cities\n",
    "filtered_df = merged_df2[merged_df2['city'].isin(top_cities)]\n",
    "\n",
    "# Create the bar chart\n",
    "plt.figure(figsize=(10, 6), dpi=200)\n",
    "ax = sns.countplot(data=filtered_df, x='city', hue='Personal Loan', palette=custom_palette)\n",
    "plt.title('Top 20 Cities by Personal Loan Distribution', fontsize=12, fontweight='bold')\n",
    "plt.xlabel('City')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=90, ha='right')\n",
    "plt.grid(True)\n",
    "\n",
    "# Set color and thickness of spines\n",
    "for spine in ['top', 'right', 'left', 'bottom']:\n",
    "    ax.spines[spine].set_color('lightgray')\n",
    "    ax.spines[spine].set_linewidth(1.2)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b0a18b-d5bc-4a2b-975d-188dbb32a2ab",
   "metadata": {},
   "source": [
    "## <b> Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c131beb-ffa1-4b01-9f3e-70ccfe8d322a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into features (X) and target (y)\n",
    "x = merged_df.drop('Personal Loan', axis=1)\n",
    "y = merged_df['Personal Loan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690cda71-75c2-43a0-9a07-ad75f96bcec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 1. Defining columns based on distribution type\n",
    "# ---------------------------\n",
    "\n",
    "# Continuous features with a normal distribution\n",
    "normal_features = ['Age', 'Experience']\n",
    "\n",
    "# Continuous features with outliers and skewed data\n",
    "skewed_features = ['Income', 'CCAvg_Annual', 'Mortgage']\n",
    "\n",
    "# Discrete or categorical features\n",
    "categorical_features = ['Family', 'Education']\n",
    "\n",
    "# Binary features that do not require normalization\n",
    "binary_features = ['Securities Account', 'CD Account', 'Online', 'CreditCard']\n",
    "\n",
    "# Geographic features to be normalized\n",
    "geo_features = ['lat', 'lng']\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Defining transformers \n",
    "# ---------------------------\n",
    "\n",
    "# Log transform + robust scaler for skewed numerical data\n",
    "log_and_robust = Pipeline(steps=[\n",
    "    ('log', FunctionTransformer(func=lambda x: np.log1p(x))),\n",
    "    ('robust', RobustScaler())\n",
    "])\n",
    "\n",
    "# Standard scaler for normally distributed and geo data\n",
    "standard = StandardScaler()\n",
    "# One-hot encoder for categorical features\n",
    "categorical = OneHotEncoder(drop=None, sparse_output=False)\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Combine all in ColumnTransformer\n",
    "# ---------------------------\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('normals', standard, normal_features),\n",
    "    ('skewed', log_and_robust, skewed_features),\n",
    "    ('categorical', categorical, categorical_features),\n",
    "    ('geo', standard, geo_features),\n",
    "    ('passthrough', 'passthrough', binary_features)\n",
    "])\n",
    "\n",
    "# Fit and transform\n",
    "X_preprocessed = preprocessor.fit_transform(x)\n",
    "\n",
    "# Save the preprocessor as the scaler for Logistic Regression and knn\n",
    "scaler_log_reg = preprocessor\n",
    "scaler_knn = preprocessor\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Create final DataFrame with column names\n",
    "# ---------------------------\n",
    "\n",
    "output_columns = (\n",
    "    normal_features +\n",
    "    skewed_features +\n",
    "    list(preprocessor.named_transformers_['categorical'].get_feature_names_out(categorical_features)) +\n",
    "    geo_features +\n",
    "    binary_features\n",
    ")\n",
    "\n",
    "final_df = pd.DataFrame(X_preprocessed, columns=output_columns)\n",
    "final_df['Personal Loan'] = y.values\n",
    "\n",
    "print(final_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47f9f31-d8d1-465d-b1b0-bbf4048d259a",
   "metadata": {},
   "source": [
    "## <b> Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67839249-8236-4014-b018-837a0da6e5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into features (X) and target (y)\n",
    "x = final_df.drop('Personal Loan', axis=1)\n",
    "y = final_df['Personal Loan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02e58ec-266d-46b4-88f1-d99dca4dd86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing multiple test sizes to find the best value\n",
    "test_size_list = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "\n",
    "test_sizes = []\n",
    "test_aucs = []\n",
    "train_aucs = []\n",
    "\n",
    "for i in test_size_list:\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=i, random_state=0, stratify=y)\n",
    "    # Initialize and train logistic regression model\n",
    "    log_reg_model = LogisticRegression(random_state=0, max_iter=1000, class_weight='balanced') # Handles class imbalance\n",
    "    log_reg_model.fit(x_train, y_train)\n",
    "    \n",
    "    # Predicting probabilities for the training and test sets\n",
    "    train_proba = log_reg_model.predict_proba(x_train)[:, 1]\n",
    "    test_proba = log_reg_model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "    # Calculating the AUC score for the training and test sets\n",
    "    train_auc = roc_auc_score(y_train, train_proba)\n",
    "    test_auc = roc_auc_score(y_test, test_proba)\n",
    "\n",
    "    # Store results\n",
    "    test_sizes.append(i)\n",
    "    train_aucs.append(train_auc)\n",
    "    test_aucs.append(test_auc)\n",
    "    # Predict binary labels for classification report\n",
    "    y_pred = log_reg_model.predict(x_test)\n",
    "\n",
    "    print(f\"\\n🔶 Test size: {i}\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"ROC-AUC Score:\", roc_auc_score(y_test, log_reg_model.predict_proba(x_test)[:, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9baf568-4d96-4d4b-917e-fcbdc04aeae2",
   "metadata": {},
   "source": [
    "🔶 Best test size is 0.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ccc26d-8bcd-4735-bcb1-ebc3b9d8eca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best test size based on the highest test AUC\n",
    "best_index = np.argmax(test_aucs)\n",
    "best_test_size = test_sizes[best_index]\n",
    "best_test_auc = test_aucs[best_index]\n",
    "\n",
    "# Plotting the chart\n",
    "plt.figure(figsize=(10, 4), dpi=200)\n",
    "plt.plot(test_sizes, train_aucs, marker='o', label='Train ROC-AUC', color='#90BE6D')\n",
    "plt.plot(test_sizes, test_aucs, marker='s', label='Test ROC-AUC', color='#EA9010')\n",
    "\n",
    "# Annotate the best test size\n",
    "plt.annotate(\n",
    "    f'Best Test Size = {best_test_size}',\n",
    "    xy=(best_test_size, best_test_auc),\n",
    "    xytext=(best_test_size - 0.05, best_test_auc - 0.03),\n",
    "    arrowprops=dict(facecolor='blue', edgecolor='blue', arrowstyle='->', lw=1.5),\n",
    "    fontsize=10, color='blue', fontweight='bold'\n",
    ")\n",
    "\n",
    "# Setting font sizes and layout\n",
    "plt.title('Train vs Test ROC-AUC Across Different Test Sizes for Logistic Regression', fontsize=12, fontweight='bold')\n",
    "plt.xlabel('Test Size', fontsize=10)\n",
    "plt.ylabel('ROC-AUC Score', fontsize=10)\n",
    "\n",
    "plt.ylim(0.90, 1.00)\n",
    "plt.legend(fontsize=8)\n",
    "plt.grid(True)\n",
    "plt.xticks(fontsize=8)\n",
    "plt.yticks(fontsize=8)\n",
    "\n",
    "# Customize spines\n",
    "for spine in plt.gca().spines.values():\n",
    "    spine.set_color('lightgray')\n",
    "    spine.set_linewidth(1.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3372e7a6-e470-44b3-9e53-8d24c5e389db",
   "metadata": {},
   "source": [
    "🔶 Test Size = 0.1\n",
    "\n",
    "Train ROC-AUC ≈ 0.972\n",
    "\n",
    "Test ROC-AUC ≈ 0.979\n",
    "\n",
    "The difference between Train and Test is small and positive. The model performs slightly better on the test set, which may suggest slight underfitting, but overall indicates good generalization.\n",
    "\n",
    "🔶 Test Size = 0.2\n",
    "\n",
    "Train ROC-AUC ≈ 0.972\n",
    "\n",
    "Test ROC-AUC ≈ 0.972\n",
    "\n",
    "Both values are nearly identical. The model shows a good balance between training and testing performance, indicating stable behavior.\n",
    "\n",
    "🔶 Test Size = 0.3\n",
    "\n",
    "Train ROC-AUC ≈ 0.971\n",
    "\n",
    "Test ROC-AUC ≈ 0.978\n",
    "\n",
    "The test ROC-AUC is higher again. Despite a slight drop in training performance, the model generalizes well, suggesting improved robustness.\n",
    "\n",
    "⭐ Test Size = 0.4 ←(←(Best Point)\n",
    "\n",
    "Train ROC-AUC ≈ 0.969\n",
    "\n",
    "Test ROC-AUC ≈ 0.979\n",
    "\n",
    "This test size yields the highest ROC-AUC on the test set. Although the gap between Train and Test increases, it’s still acceptable. No clear signs of overfitting.\n",
    "\n",
    "This point shows the best generalization performance.\n",
    "\n",
    "🔶 Test Size = 0.5\n",
    "\n",
    "Train ROC-AUC ≈ 0.968\n",
    "\n",
    "Test ROC-AUC ≈ 0.978\n",
    "\n",
    "Test performance remains high, but training ROC-AUC drops slightly more. Still a good balance, though slightly less ideal than at 0.4.\n",
    "\n",
    "🔶 Conclusion:\n",
    "\n",
    "Test Size = 0.4 is the best choice, providing the highest test ROC-AUC.\n",
    "\n",
    "The model demonstrates excellent generalization and performance at this split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f79529e-8118-478d-9892-20206b282a0d",
   "metadata": {},
   "source": [
    "### <b> Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47329988-4769-4813-abe9-98abade8d68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "class_weights = [None, 'balanced']\n",
    "C_range = [0.01, 0.1, 1, 10, 100]\n",
    "l1_ratios = np.arange(0, 1.1, 0.2)\n",
    "\n",
    "# Create parameter grid covering different solvers and penalties\n",
    "param_grid = [\n",
    "    {'solver': ['lbfgs', 'newton-cg', 'sag', 'saga'], \n",
    "     'penalty': ['none'], \n",
    "     'class_weight': class_weights, \n",
    "     'max_iter': [500, 1000]\n",
    "    },\n",
    "    {'solver': ['lbfgs', 'newton-cg', 'sag'], \n",
    "     'penalty': ['l2'], \n",
    "     'C': C_range, \n",
    "     'class_weight': class_weights, \n",
    "     'max_iter': [500, 1000]\n",
    "    },\n",
    "    {'solver': ['liblinear'], \n",
    "     'penalty': ['l1', 'l2'], \n",
    "     'C': C_range, \n",
    "     'class_weight': class_weights, \n",
    "     'max_iter': [500, 1000]\n",
    "    },\n",
    "    {'solver': ['saga'], \n",
    "     'penalty': ['l1', 'l2'], \n",
    "     'C': C_range, \n",
    "     'class_weight': class_weights, \n",
    "     'max_iter': [500, 1000]\n",
    "    },\n",
    "    {'solver': ['saga'], \n",
    "     'penalty': ['elasticnet'], \n",
    "     'C': C_range, \n",
    "     'l1_ratio': l1_ratios, \n",
    "     'class_weight': class_weights, \n",
    "     'max_iter': [500, 1000]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Try different CV values and store results\n",
    "cv_values = [3, 5, 7, 10]\n",
    "cv_results = []\n",
    "\n",
    "# Base model\n",
    "log_reg_model = LogisticRegression(random_state=0)\n",
    "\n",
    "for cv in cv_values:\n",
    "    print(f\"\\n🔶 Running GridSearchCV with cv={cv}\")\n",
    "    grid = GridSearchCV(\n",
    "        estimator=log_reg_model,\n",
    "        param_grid=param_grid,\n",
    "        cv=StratifiedKFold(n_splits=cv, shuffle=True, random_state=0),\n",
    "        scoring='roc_auc',\n",
    "        return_train_score=True,\n",
    "        n_jobs=-1,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    grid.fit(x, y)\n",
    "\n",
    "    # Extract best index and corresponding metrics\n",
    "    best_idx = grid.best_index_\n",
    "    train_auc = grid.cv_results_['mean_train_score'][best_idx]\n",
    "    val_auc = grid.cv_results_['mean_test_score'][best_idx]\n",
    "\n",
    "    # Save result\n",
    "    cv_results.append({\n",
    "        'cv': cv,\n",
    "        'grid': grid,\n",
    "        'train_auc': train_auc,\n",
    "        'val_auc': val_auc\n",
    "    })\n",
    "\n",
    "    print(f\" cv={cv} ➤ Train ROC-AUC: {train_auc:.4f}, Validation ROC-AUC: {val_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26deb30c-7416-4936-8b9e-5d603d7102fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables to track the best results\n",
    "best_cv = None\n",
    "best_model = None\n",
    "best_score = 0\n",
    "\n",
    "# Loop through cv_results to find the best ROC-AUC score\n",
    "for result in cv_results:\n",
    "    if result['val_auc'] > best_score:\n",
    "        best_score = result['val_auc']\n",
    "        best_cv = result['cv']\n",
    "        best_model = result['grid']\n",
    "        \n",
    "print(\"\\n🔶 Best number of folds (cv):\", best_cv)\n",
    "print(\"🔶 The best combination of hyperparameters:\", best_model.best_params_)\n",
    "print(\"🔶 Highest ROC-AUC score:\", best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67fd5fd-df77-4a6a-9bbf-d8ca81b859e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting data for plotting\n",
    "cv_vals = [r['cv'] for r in cv_results]\n",
    "train_auc_scores = [r['train_auc'] for r in cv_results]\n",
    "val_auc_scores = [r['val_auc'] for r in cv_results]\n",
    "\n",
    "# Find the best CV fold (based on validation AUC)\n",
    "best_idx = np.argmax(val_auc_scores)\n",
    "best_cv = cv_vals[best_idx]\n",
    "best_val_auc = val_auc_scores[best_idx]\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 4), dpi=200)\n",
    "plt.plot(cv_vals, train_auc_scores, marker='o', label='Train ROC-AUC', color='#90BE6D')\n",
    "plt.plot(cv_vals, val_auc_scores, marker='s', label='Validation ROC-AUC', color='#EA9010')\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel('CV folds (k)', fontsize=10)\n",
    "plt.ylabel('ROC-AUC Score', fontsize=10)\n",
    "plt.title('Train vs Validation ROC-AUC Across Different CV Values for Logistic Regression', fontsize=12, fontweight='bold')\n",
    "plt.ylim(min(min(train_auc_scores), min(val_auc_scores)) - 0.01, 1.0)\n",
    "plt.xticks(cv_vals)\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# Annotate best CV\n",
    "plt.annotate(\n",
    "    f'Best CV = {best_cv}', \n",
    "    xy=(best_cv, best_val_auc), \n",
    "    xytext=(best_cv - 0.4, best_val_auc + 0.015), \n",
    "    arrowprops=dict(facecolor='blue', edgecolor='blue', arrowstyle='->', lw=1.5),\n",
    "    fontsize=10, color='blue', fontweight='bold'\n",
    ")\n",
    "\n",
    "plt.xticks(fontsize=8)\n",
    "plt.yticks(fontsize=8)\n",
    "for spine in plt.gca().spines.values():\n",
    "    spine.set_color('lightgray')\n",
    "    spine.set_linewidth(1.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e4a4dd-5821-4286-a61d-6a9e5fc90c92",
   "metadata": {},
   "source": [
    "🔶 CV = 3\n",
    "\n",
    "Train ROC-AUC ≈ 0.973\n",
    "\n",
    "Validation ROC-AUC ≈ 0.970\n",
    "\n",
    "A small gap is present, but both scores are high. This suggests decent performance but possibly higher variance due to fewer folds.\n",
    "\n",
    "🔶 CV = 5\n",
    "\n",
    "Train ROC-AUC ≈ 0.973\n",
    "\n",
    "Validation ROC-AUC ≈ 0.970\n",
    "\n",
    "Very similar to CV=3. No significant improvement, but the model remains stable.\n",
    "\n",
    "🔶 CV = 7\n",
    "\n",
    "Train ROC-AUC ≈ 0.973\n",
    "\n",
    "Validation ROC-AUC ≈ 0.971\n",
    "\n",
    "A slight increase in validation ROC-AUC is observed, suggesting better performance with more folds.\n",
    "\n",
    "⭐ CV = 10 ←(Best point)\n",
    "\n",
    "Train ROC-AUC ≈ 0.973\n",
    "\n",
    "Validation ROC-AUC ≈ 0.9715\n",
    "\n",
    "This value gives the highest validation ROC-AUC.\n",
    "\n",
    "The train-validation gap is minimal, indicating excellent generalization.\n",
    "\n",
    "Best choice for cross-validation split in this experiment.\n",
    "\n",
    "🔶 Conclusion:\n",
    "\n",
    "CV = 10 provides the most reliable validation score and lowest variance.\n",
    "\n",
    "The model generalizes consistently across all folds, showing strong robustness and stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b5e02b-bd9b-4f4e-8e6d-222c49d1f784",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = cv_results[cv_vals.index(10)]['grid']\n",
    "results = grid.cv_results_\n",
    "\n",
    "# Filter only rows where model used:\n",
    "# - L1 penalty\n",
    "# - liblinear solver\n",
    "# - balanced class weight\n",
    "# - 500 iterations\n",
    "filtered_idxs = [\n",
    "    i for i, params in enumerate(results['params'])\n",
    "    if params.get('penalty') == 'l1' and\n",
    "       params.get('solver') == 'liblinear' and\n",
    "       params.get('class_weight') == 'balanced' and\n",
    "       params.get('max_iter') == 500\n",
    "]\n",
    "\n",
    "# Extract values of C and their corresponding scores\n",
    "C_vals = [results['params'][i]['C'] for i in filtered_idxs]\n",
    "val_scores = [results['mean_test_score'][i] for i in filtered_idxs]\n",
    "train_scores = [results['mean_train_score'][i] for i in filtered_idxs]\n",
    "\n",
    "# Identify best C based on validation ROC-AUC\n",
    "best_idx = np.argmax(val_scores)\n",
    "best_C = C_vals[best_idx]\n",
    "best_val_score = val_scores[best_idx]\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 4), dpi=200)\n",
    "plt.plot(C_vals, train_scores, marker='o', label='Train ROC-AUC', color='#90BE6D')\n",
    "plt.plot(C_vals, val_scores, marker='s', label='Validation ROC-AUC', color='#EA9010')\n",
    "\n",
    "plt.xscale('log')  # تبدیل مقیاس محور x به لگاریتمی\n",
    "plt.xlabel('C (log scale)', fontsize=10)\n",
    "plt.ylabel('ROC-AUC Score', fontsize=10)\n",
    "plt.title('Train vs Validation ROC-AUC Across Different C values\\n(CV=10, Best Params)', fontsize=12, fontweight='bold')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# Annotate best C value on the chart\n",
    "plt.annotate(\n",
    "    f'Best C = {best_C}',\n",
    "    xy=(best_C, best_val_score),\n",
    "    xytext=(best_C, best_val_score - 0.02),\n",
    "    arrowprops=dict(facecolor='blue', edgecolor='blue', arrowstyle='->', lw=1.5),\n",
    "    fontsize=10, color='blue', fontweight='bold'\n",
    ")\n",
    "\n",
    "# Customize Y-axis limits\n",
    "plt.ylim(min(min(train_scores), min(val_scores)) - 0.01, 1.0)\n",
    "\n",
    "for spine in plt.gca().spines.values():\n",
    "    spine.set_color('lightgray')\n",
    "    spine.set_linewidth(1.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d1be86-645c-48a0-bd29-7652e5531c7f",
   "metadata": {},
   "source": [
    "🔶 C is the inverse of regularization strength in Logistic Regression. Smaller values imply stronger regularization.\n",
    "\n",
    "🔶 This chart examines how different values of C affect the ROC-AUC score on both training and validation sets.\n",
    "\n",
    "🔶 C = 0.01\n",
    "\n",
    "Train ROC-AUC ≈ 0.960\n",
    "\n",
    "Validation ROC-AUC ≈ 0.958\n",
    "\n",
    "Both scores are the lowest among the tested values.\n",
    "\n",
    "Indicates underfitting — the model is too constrained and unable to learn enough from the data.\n",
    "\n",
    "⭐ C = 0.1 ←(Best Point)\n",
    "\n",
    "Train ROC-AUC ≈ 0.972\n",
    "\n",
    "Validation ROC-AUC ≈ 0.971\n",
    "\n",
    "This is where validation ROC-AUC is highest, and the train-validation gap is minimal.\n",
    "\n",
    "Suggests a good balance between bias and variance.\n",
    "\n",
    "Best value for C, as it provides strong generalization with optimal complexity.\n",
    "\n",
    "🔶 C = 1, 10, 100\n",
    "\n",
    "Train ROC-AUC stays slightly above 0.973\n",
    "\n",
    "Validation ROC-AUC slightly decreases (~0.970)\n",
    "\n",
    "The model starts overfitting slightly: it performs better on the training set but not better on validation.\n",
    "\n",
    "Increasing C reduces regularization, allowing the model to fit training data more closely — but not benefiting test performance.\n",
    "\n",
    "\n",
    "🔶 C = 0.1 is the most suitable choice.\n",
    "\n",
    "🔶 It gives the highest validation ROC-AUC, with a minimal gap from training ROC-AUC.\n",
    "\n",
    "🔶 This value ensures the model is neither too simple nor too complex — ideal for generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c50a63-351b-4de3-9149-032f9d38d5d2",
   "metadata": {},
   "source": [
    "### <b> Final Training and Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d0c0a7-1daf-43c4-9e5d-888cf8cc6d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=best_test_size, random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801ea964-5a97-4461-bcfb-c3f623f7602f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a Logistic Regression model with the best C value obtained from GridSearchCV\n",
    "log_reg_model = LogisticRegression(random_state=0, class_weight='balanced')\n",
    "\n",
    "# Load the best model found from GridSearchCV\n",
    "best_log_reg_model = grid.best_estimator_\n",
    "print(\"Best Parameters:\", grid.best_params_)\n",
    "\n",
    "# Predicting the probability of each test sample belonging to the positive class (1)\n",
    "y_score = best_log_reg_model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "# Calculating the values needed to plot the ROC curve: False Positive Rate, True Positive Rate\n",
    "fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "roc_auc = auc(fpr, tpr) # Calculating the Area Under the ROC Curve (ROC-AUC)\n",
    "\n",
    "# Calculating the values needed to plot the Precision-Recall curve\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_score)\n",
    "avg_prec = average_precision_score(y_test, y_score) # Average Precision (AP)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5), dpi=150)\n",
    "\n",
    "# ROC Curve\n",
    "axes[0].plot(fpr, tpr, label=f'ROC AUC = {roc_auc:.2f}', color='#90BE6D')\n",
    "axes[0].plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "axes[0].set_xlabel('False Positive Rate', fontsize=10)\n",
    "axes[0].set_ylabel('True Positive Rate', fontsize=10)\n",
    "axes[0].set_title('ROC Curve', fontsize=12, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Precision-Recall Curve\n",
    "axes[1].plot(recall, precision, label=f'AP = {avg_prec:.2f}', color='#EA9010')\n",
    "axes[1].set_xlabel('Recall', fontsize=10)\n",
    "axes[1].set_ylabel('Precision', fontsize=10)\n",
    "axes[1].set_title('Precision-Recall Curve', fontsize=12, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "for ax in axes:\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_edgecolor('#D3D3D3') \n",
    "        spine.set_linewidth(1.5)\n",
    " \n",
    "plt.suptitle('Performance Analysis of Tuned Logistic Regression Model', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd4ae3d-504b-48f6-9009-b36973b85107",
   "metadata": {},
   "source": [
    "🔶 ROC Curve (Receiver Operating Characteristic)\n",
    "\n",
    "AUC (Area Under Curve) = 0.98\n",
    "\n",
    "The ROC curve shows a very steep rise towards the top-left corner, indicating a strong trade-off between True Positive Rate (TPR) and False Positive Rate (FPR).\n",
    "\n",
    "AUC = 0.98 is considered excellent — meaning the model is highly capable of distinguishing between the two classes (loan vs no-loan).\n",
    "\n",
    "The curve being far from the diagonal line (random guess line) confirms that the model performs significantly better than random chance.\n",
    "\n",
    "Interpretation:\n",
    "The model has excellent discrimination ability. It rarely misclassifies positive and negative samples.\n",
    "\n",
    "🔶 Precision-Recall Curve\n",
    "\n",
    "AP (Average Precision) = 0.87\n",
    "\n",
    "The curve maintains high precision at various levels of recall, especially in the 0.0–0.6 recall range.\n",
    "\n",
    "A gradual decline in precision at higher recall levels (beyond 0.6) indicates some trade-off between catching all positives and avoiding false positives.\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "A high AP score suggests that the model is effective even in imbalanced scenarios (if applicable), maintaining a good balance between identifying true positives and avoiding false alarms.\n",
    "\n",
    "🔶 Overall Conclusion:\n",
    "\n",
    "ROC AUC = 0.98 → Excellent general classification performance.\n",
    "\n",
    "AP = 0.87 → Strong performance in ranking and probability calibration.\n",
    "\n",
    "The model is reliable for both balanced and slightly imbalanced class distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7b5184-c7c4-41fb-a091-c1a5370eb994",
   "metadata": {},
   "source": [
    "### <b> Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e24d7e3-252b-4ac4-9d10-89d48da0a3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_log_reg_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b47286-7705-4dd6-95b0-827779a923d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculation and display of evaluation metrics\n",
    "print(\"🔶 Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"🔶 Precision:\", precision_score(y_test, y_pred, average='weighted'))\n",
    "print(\"🔶 Recall:\", recall_score(y_test, y_pred, average='weighted'))\n",
    "print(\"🔶 F1 Score:\", f1_score(y_test, y_pred, average='weighted'))\n",
    "print(\"\\n🔶 Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion Matrix Plot\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "fig, ax = plt.subplots(figsize=(4, 4), dpi=150)  \n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', cbar=False, square=True,\n",
    "            annot_kws={\"size\": 10}, ax=ax)\n",
    "\n",
    "ax.set_xlabel(\"Predicted Label\", fontsize=10)\n",
    "ax.set_ylabel(\"True Label\", fontsize=10)\n",
    "ax.set_title(\"Confusion Matrix\", fontsize=12, fontweight='bold')\n",
    "\n",
    "ax.tick_params(axis='both', labelsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd0339a-9e83-451f-aefe-ad845d25f967",
   "metadata": {},
   "source": [
    "🔶 Overall Metrics\n",
    "\n",
    "🔶 Accuracy: 89.76%\n",
    "\n",
    "→ The model correctly predicts ~90% of total instances.\n",
    "\n",
    "🔶 Precision: 95.08%\n",
    "\n",
    "→ When the model predicts a person won't take a loan (class 0), it's correct 95% of the time.\n",
    "\n",
    "🔶 Recall: 89.75%\n",
    "\n",
    "→ The model identifies 89.75% of all actual class 0 and class 1 correctly.\n",
    "\n",
    "🔶 F1 Score: 91.31%\n",
    "\n",
    "→ Balanced measure of precision and recall, showing strong performance.\n",
    "\n",
    "🔶 Class 0 (No Loan):\n",
    "\n",
    "Very high precision (1.00): No false positives.\n",
    "\n",
    "Good recall (0.89): Most non-loan applicants are detected.\n",
    "\n",
    "🔶 Class 1 (Loan):\n",
    "\n",
    "Low precision (0.44): A lot of false positives — the model often wrongly predicts loan takers.\n",
    "\n",
    "High recall (0.95): Almost all actual loan takers are caught.\n",
    "\n",
    "🔶 Implication: The model is very cautious — it prefers to catch all loan takers (high recall), even at the cost of wrongly labeling many non-loan users as loan-takers (low precision for class 1).\n",
    "\n",
    "🔶 Needs improvement in precision for class 1, possibly through threshold tuning, resampling, or cost-sensitive learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e551d9-bef2-48cd-b067-22b295eb1eff",
   "metadata": {},
   "source": [
    "### <b> Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2413b514-530b-4624-8186-b2fceb2f6392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a color palette between yellow and green\n",
    "colors = sns.color_palette(\"YlGn\", n_colors=10)  # Yellow to Green\n",
    "\n",
    "# Extracting model coefficients (i.e., feature importance in Logistic Regression)\n",
    "importance = best_log_reg_model.coef_[0]\n",
    "features = (\n",
    "    normal_features +\n",
    "    skewed_features +\n",
    "    list(preprocessor.named_transformers_['categorical'].get_feature_names_out(categorical_features)) +\n",
    "    geo_features +\n",
    "    binary_features\n",
    ") # Since there is only one output class, coef_ has shape (1, n_features)\n",
    "\n",
    "# Create a DataFrame of features and their corresponding importance\n",
    "coeff_df = pd.DataFrame({'Feature': features, 'Importance': importance})\n",
    "coeff_df['AbsImportance'] = np.abs(coeff_df['Importance']) # Add a new column with the absolute value of importance for sorting\n",
    "coeff_df.sort_values('AbsImportance', ascending=False, inplace=True) # Sort in descending order based on the absolute importance\n",
    "\n",
    "plt.figure(figsize=(10, 4), dpi=200)\n",
    "sns.barplot(x='Importance', y='Feature', data=coeff_df.head(10), palette=colors)\n",
    "plt.title('Top 10 Important Features', fontsize=12, fontweight='bold') \n",
    "\n",
    "for spine in plt.gca().spines.values():\n",
    "    spine.set_edgecolor('#D3D3D3') \n",
    "    spine.set_linewidth(1.5)  \n",
    "\n",
    "plt.xlabel('Importance', fontsize=10)  \n",
    "plt.ylabel('Feature', fontsize=10)   \n",
    "\n",
    "plt.tick_params(axis='x', labelsize=8)  \n",
    "plt.tick_params(axis='y', labelsize=8)  \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463c1699-c7d0-47c3-ba91-32d3bfb06354",
   "metadata": {},
   "source": [
    "🔶 Strongest predictors:\n",
    "\n",
    "Income and CD Account are dominant. People with higher incomes and more financial assets are much more likely to take loans (possibly for investments or larger purchases).\n",
    "\n",
    "🔶Education and Family:\n",
    "\n",
    "education level 1 and smaller family sizes are negatively associated, which could suggest lower financial need or risk aversion.\n",
    "\n",
    "🔶 Spending & financial tools:\n",
    "\n",
    "Features like CCAvg_Annual, CreditCard, and Securities Account reflect user activity and financial engagement, mildly influencing loan prediction.\n",
    "\n",
    "🔶 Takeaway\n",
    "Your model heavily relies on income and financial asset indicators, followed by demographics (like education and family) and spending behavior. This is generally intuitive and aligned with real-world loan behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185c1631-fcfc-43a3-9467-3aa60db1070a",
   "metadata": {},
   "source": [
    "## <b> KNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf69e62-74e3-4368-a43e-f784676a3c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty lists to store results\n",
    "test_sizes_knn = []\n",
    "test_aucs_knn = []\n",
    "train_aucs_knn = []\n",
    "\n",
    "for i in test_size_list:\n",
    "    x_train_knn, x_test_knn, y_train_knn, y_test_knn = train_test_split(x, y, test_size=i, random_state=0)\n",
    "\n",
    "    # Create KNN model with n_neighbors=11 (default for KNN classifier)\n",
    "    knn_model = KNeighborsClassifier(n_neighbors=27)\n",
    "    knn_model.fit(x_train_knn, y_train_knn) # Fit model on the training data\n",
    "\n",
    "    # Predicting probabilities\n",
    "    train_proba_knn = knn_model.predict_proba(x_train_knn)[:, 1] # Probability for the positive class in training set\n",
    "    test_proba_knn = knn_model.predict_proba(x_test_knn)[:, 1]   # Probability for the positive class in test set\n",
    "    \n",
    "    # Calculating ROC-AUC\n",
    "    train_auc_knn = roc_auc_score(y_train_knn, train_proba_knn)\n",
    "    test_auc_knn = roc_auc_score(y_test_knn, test_proba_knn)\n",
    "\n",
    "    # Append results to the lists\n",
    "    test_sizes_knn.append(i)\n",
    "    train_aucs_knn.append(train_auc_knn)\n",
    "    test_aucs_knn.append(test_auc_knn)\n",
    "\n",
    "    # Predict labels for the test set (for classification report and confusion matrix)\n",
    "    y_pred_knn = knn_model.predict(x_test_knn)\n",
    "\n",
    "    # Output the results for the current test size\n",
    "    print(f\"\\n🔶 Test size: {i}\")\n",
    "    print(confusion_matrix(y_test_knn, y_pred_knn))\n",
    "    print(classification_report(y_test_knn, y_pred_knn))\n",
    "    print(\"ROC-AUC Score:\", roc_auc_score(y_test_knn, knn_model.predict_proba(x_test_knn)[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bac7c0-e147-450d-8b83-01b563e6dffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best test size based on the highest test AUC\n",
    "best_index_knn = np.argmax(test_aucs_knn)\n",
    "best_test_size_knn = test_sizes_knn[best_index_knn]\n",
    "best_test_auc_knn = test_aucs_knn[best_index_knn]\n",
    "\n",
    "# Plotting the same chart\n",
    "plt.figure(figsize=(10, 4), dpi=200)\n",
    "plt.plot(test_sizes_knn, train_aucs_knn, marker='o', label='Train ROC-AUC', color='#90BE6D')\n",
    "plt.plot(test_sizes_knn, test_aucs_knn, marker='s', label='Test ROC-AUC', color='#EA9010')\n",
    "\n",
    "plt.annotate(\n",
    "    f'Best Test Size = {best_test_size_knn}',\n",
    "    xy=(best_test_size_knn, best_test_auc_knn),\n",
    "    xytext=(best_test_size_knn + 0.05, best_test_auc_knn + 0.01),\n",
    "    arrowprops=dict(facecolor='blue', edgecolor='blue', arrowstyle='->', lw=1.5),\n",
    "    fontsize=10, color='blue', fontweight='bold'\n",
    ")\n",
    "\n",
    "plt.title('Train vs Test ROC-AUC Across Different Test Sizes for KNN', fontsize=12, fontweight='bold')\n",
    "plt.xlabel('Test Size', fontsize=10)\n",
    "plt.ylabel('ROC-AUC Score', fontsize=10)\n",
    "\n",
    "plt.ylim(0.85, 1.00)\n",
    "plt.legend(fontsize=8)\n",
    "plt.grid(True)\n",
    "plt.xticks(fontsize=8)\n",
    "plt.yticks(fontsize=8)\n",
    "\n",
    "# Customize spines\n",
    "for spine in plt.gca().spines.values():\n",
    "    spine.set_color('lightgray')\n",
    "    spine.set_linewidth(1.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac19483e-60ed-466b-9ea5-8645371402e8",
   "metadata": {},
   "source": [
    "🔸 Test Size = 0.10\n",
    "\n",
    "Train ROC-AUC ≈ 0.976\n",
    "\n",
    "Test ROC-AUC ≈ 0.935\n",
    "\n",
    "Significant gap: The model performs much better on training than testing — signs of overfitting.\n",
    "\n",
    "🔸 Test Size = 0.20\n",
    "\n",
    "Train ROC-AUC ≈ 0.975\n",
    "\n",
    "Test ROC-AUC ≈ 0.944\n",
    "\n",
    "Slight improvement in test performance, but gap remains noticeable.\n",
    "\n",
    "🔸 Test Size = 0.30\n",
    "\n",
    "Train ROC-AUC ≈ 0.976\n",
    "\n",
    "Test ROC-AUC ≈ 0.946\n",
    "\n",
    "Test performance increases slightly; the model still likely overfits somewhat.\n",
    "\n",
    "⭐ Test Size = 0.40 ←(Best Point)\n",
    "\n",
    "Train ROC-AUC ≈ 0.976\n",
    "\n",
    "Test ROC-AUC ≈ 0.955 ← Highest test performance\n",
    "\n",
    "Best generalization point: relatively low gap, best validation result.\n",
    "\n",
    "🔸 Test Size = 0.50\n",
    "\n",
    "Train ROC-AUC ≈ 0.976\n",
    "\n",
    "Test ROC-AUC ≈ 0.954\n",
    "\n",
    "Slight drop compared to 0.4. Still close, but not optimal.\n",
    "\n",
    "🔸 Conclusion\n",
    "\n",
    "Best Test Size = 0.4, as it gives the highest Test ROC-AUC (≈ 0.955).\n",
    "\n",
    "The gap between Train and Test scores narrows as test size increases — a positive sign for model generalization.\n",
    "\n",
    "At smaller test sizes (e.g., 0.1–0.2), the KNN model shows overfitting.\n",
    "\n",
    "With 40% test data, the model is better balanced and generalizes well to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65282d9-3958-467c-9cd3-afc3366bad0c",
   "metadata": {},
   "source": [
    "### <b> Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5aa5aa-381e-482a-b83f-e3b4f4f08869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter distributions for RandomizedSearchCV\n",
    "param_distributions_knn = {\n",
    "    'n_neighbors': randint(3, 30), # Number of neighbors to use (randomized between 3 and 30)\n",
    "    'weights': ['uniform', 'distance'],  # 'uniform' or 'distance' weight function for neighbors\n",
    "    'algorithm': ['auto', 'brute'], # Algorithm used to compute nearest neighbors: 'auto' or 'brute'\n",
    "    # 'ball_tree' and 'kd_tree' algorithms may work better with numeric data, but we use 'auto' and 'brute' here for simplicity\n",
    "    'leaf_size': randint(20, 41), # Size of leaf nodes for tree-based algorithms (randomized between 20 and 41)\n",
    "    'metric': ['minkowski', 'euclidean', 'manhattan', 'chebyshev'], # Distance metrics for KNN\n",
    "    'p': randint(1, 3)  # Only effective when metric='minkowski', p=1 for Manhattan distance, p=2 for Euclidean distance\n",
    "}\n",
    "\n",
    "# List to store results of different cross-validation settings\n",
    "cv_results_knn = []\n",
    "\n",
    "# Initialize the KNN classifier model\n",
    "knn_model = KNeighborsClassifier()\n",
    "\n",
    "for cv in cv_values:\n",
    "    print(f\"\\n🔶 Running RandomizedSearchCV for KNN with cv={cv}\")\n",
    "    # Initialize RandomizedSearchCV with specified parameters\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=knn_model,\n",
    "        param_distributions=param_distributions_knn,\n",
    "        n_iter=30,\n",
    "        cv=StratifiedKFold(n_splits=cv, shuffle=True, random_state=0),\n",
    "        scoring='roc_auc',\n",
    "        return_train_score=True,\n",
    "        n_jobs=-1,\n",
    "        random_state=0,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # Fit the RandomizedSearchCV with the training data\n",
    "    search.fit(x, y)\n",
    "\n",
    "    # Get the best index and corresponding AUC scores\n",
    "    best_idx_knn = search.best_index_ # Index of the best model based on validation AUC\n",
    "    train_auc_knn = search.cv_results_['mean_train_score'][best_idx_knn] # Train ROC-AUC score\n",
    "    val_auc_knn = search.cv_results_['mean_test_score'][best_idx_knn] # Validation ROC-AUC score\n",
    "\n",
    "    # Store results for each cross-validation value\n",
    "    cv_results_knn.append({\n",
    "        'cv': cv,\n",
    "        'grid': search,\n",
    "        'train_auc': train_auc_knn,\n",
    "        'val_auc': val_auc_knn\n",
    "    })\n",
    "\n",
    "    print(f\" cv={cv} ➤ Train ROC-AUC: {train_auc_knn:.4f}, Validation ROC-AUC: {val_auc_knn:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6359533-f1c6-42c2-9e30-253a6959c364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data for plotting the results\n",
    "cv_vals_knn = [r['cv'] for r in cv_results_knn]\n",
    "train_auc_scores_knn = [r['train_auc'] for r in cv_results_knn]\n",
    "val_auc_scores_knn = [r['val_auc'] for r in cv_results_knn]\n",
    "\n",
    "# Find the best cv based on the highest Validation AUC\n",
    "best_idx_knn = np.argmax(val_auc_scores_knn)\n",
    "best_cv_knn = cv_vals_knn[best_idx_knn]\n",
    "best_val_auc_knn = val_auc_scores_knn[best_idx_knn]\n",
    "best_model_knn = cv_results_knn[best_idx_knn]['grid']\n",
    "\n",
    "print(\"\\n🔶 Best number of folds (cv):\", best_cv_knn)\n",
    "print(\"🔶 best combination of hyperparameters: \", best_model_knn.best_params_)\n",
    "print(\"🔶 Highest ROC-AUC score: \", best_val_auc_knn)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8b8f3d-6c2d-4d59-8e3f-d161e831c1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting ROC-AUC scores for different CV fold values in KNN\n",
    "plt.figure(figsize=(10, 4), dpi=200)\n",
    "# Plot train ROC-AUC scores\n",
    "plt.plot(cv_vals_knn, train_auc_scores_knn, marker='o', label='Train ROC-AUC', color='#90BE6D')\n",
    "# Plot validation ROC-AUC scores\n",
    "plt.plot(cv_vals_knn, val_auc_scores_knn, marker='s', label='Validation ROC-AUC', color='#EA9010')\n",
    "\n",
    "plt.xlabel('CV folds (k)', fontsize=10)\n",
    "plt.ylabel('ROC-AUC Score', fontsize=10)\n",
    "plt.title('KNN: Train vs Validation ROC-AUC Across Different CV Values for KNN', fontsize=12, fontweight='bold')\n",
    "plt.ylim(min(min(train_auc_scores_knn), min(val_auc_scores_knn)) - 0.01, 1.0) # Y-axis range based on minimum value\n",
    "plt.xticks(cv_vals_knn) # X-axis ticks based on CV values used\n",
    "\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# Annotating the best CV fold on the plot\n",
    "plt.annotate(\n",
    "    f'Best CV = {best_cv_knn}', \n",
    "    xy=(best_cv_knn, best_val_auc_knn), \n",
    "    xytext=(best_cv_knn - 0.5, best_val_auc_knn - 0.015), \n",
    "    arrowprops=dict(facecolor='blue', edgecolor='blue', arrowstyle='->', lw=1.5),\n",
    "    fontsize=10, color='blue', fontweight='bold'\n",
    ")\n",
    "\n",
    "plt.xticks(fontsize=8)\n",
    "plt.yticks(fontsize=8)\n",
    "for spine in plt.gca().spines.values():\n",
    "    spine.set_color('lightgray')\n",
    "    spine.set_linewidth(1.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f8d557-eb04-4cb0-8b77-4f4bfdb09d8f",
   "metadata": {},
   "source": [
    "🔸 CV = 3\n",
    "\n",
    "Train ROC-AUC ≈ 0.978\n",
    "\n",
    "Validation ROC-AUC ≈ 0.952\n",
    "\n",
    "The gap is noticeable but not excessive — moderate generalization.\n",
    "\n",
    "🔸 CV = 5\n",
    "\n",
    "Train ROC-AUC spikes ≈ 1.00\n",
    "\n",
    "Validation ROC-AUC ≈ 0.958\n",
    "\n",
    "A large gap appears: the model fits the training data almost perfectly, suggesting overfitting.\n",
    "\n",
    "🔸 CV = 7\n",
    "\n",
    "Train ROC-AUC drops ≈ 0.982\n",
    "\n",
    "Validation ROC-AUC ≈ 0.957\n",
    "\n",
    "The gap narrows again, showing improved generalization.\n",
    "\n",
    "⭐ CV = 10 ←(Best Point)\n",
    "\n",
    "Train ROC-AUC ≈ 0.982\n",
    "\n",
    "Validation ROC-AUC ≈ 0.958 ← Highest validation performance\n",
    "\n",
    "Consistent and balanced — best trade-off between bias and variance.\n",
    "\n",
    "🔸 Conclusion\n",
    "\n",
    "Best CV = 10 is chosen because it achieves the highest Validation ROC-AUC while maintaining stable training performance.\n",
    "\n",
    "Lower CV values (like 3 or 5) introduce instability:\n",
    "\n",
    "CV=5 especially causes overfitting (Train ROC-AUC ≈ 1.00).\n",
    "\n",
    "Higher CV values (7 and 10) lead to better generalization and reliable evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f0c73e-5879-4c17-942f-2ea8bd6bf8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the grid corresponding to cv=10\n",
    "grid = cv_results_knn[cv_vals_knn.index(10)]['grid']\n",
    "results_knn = grid.cv_results_\n",
    "\n",
    "# Filter only rows with the same hyperparameters as the best (except n_neighbors)\n",
    "filtered_idxs_knn = [\n",
    "    i for i, params in enumerate(results_knn['params'])\n",
    "    if params.get('algorithm') == 'brute' and\n",
    "       params.get('metric') == 'manhattan' and\n",
    "       params.get('p') == 2 and\n",
    "       params.get('weights') == 'uniform'\n",
    "]\n",
    "\n",
    "\n",
    "# Check if any rows match the filtering criteria\n",
    "if not filtered_idxs_knn:\n",
    "    print(\"No rows found with the specified parameter combination.\")\n",
    "else:\n",
    "    # Extract n_neighbors and corresponding scores\n",
    "    n_vals_knn = [results_knn['params'][i]['n_neighbors'] for i in filtered_idxs_knn]\n",
    "    val_scores_knn= [results_knn['mean_test_score'][i] for i in filtered_idxs_knn]\n",
    "    train_scores_knn = [results_knn['mean_train_score'][i] for i in filtered_idxs_knn]\n",
    "\n",
    "    # Sort data based on n_neighbors for cleaner plots\n",
    "    sorted_data_knn = sorted(zip(n_vals_knn, train_scores_knn, val_scores_knn))\n",
    "    n_vals_knn, train_scores_knn, val_scores_knn = zip(*sorted_data_knn)\n",
    "\n",
    "    # Identify the best n_neighbors based on validation score\n",
    "    best_idx_knn = np.argmax(val_scores_knn)\n",
    "    best_n = n_vals_knn[best_idx_knn]\n",
    "    best_val_score = val_scores_knn[best_idx_knn]\n",
    "    \n",
    "   # Plotting Train and Validation ROC-AUC against n_neighbors\n",
    "    plt.figure(figsize=(10, 4), dpi=200)\n",
    "    plt.plot(n_vals_knn, train_scores_knn, marker='o', label='Train ROC-AUC', color='#90BE6D')\n",
    "    plt.plot(n_vals_knn, val_scores_knn, marker='s', label='Validation ROC-AUC', color='#EA9010')\n",
    "\n",
    "    plt.xlabel('n_neighbors', fontsize=10)\n",
    "    plt.ylabel('ROC-AUC Score', fontsize=10)\n",
    "    plt.title('Train vs Validation ROC-AUC Across Different n_neighbors\\n(CV=10, Similar to Best Params)', fontsize=12, fontweight='bold')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    # Annotate best n\n",
    "    plt.annotate(\n",
    "        f'Best n = {best_n}',\n",
    "        xy=(best_n, best_val_auc_knn),\n",
    "        xytext=(best_n - 1.2, best_val_auc_knn - 0.02),\n",
    "        arrowprops=dict(facecolor='blue', edgecolor='blue', arrowstyle='->', lw=1.5),\n",
    "        fontsize=10, color='blue', fontweight='bold'\n",
    "    )\n",
    "\n",
    "    plt.ylim(min(min(train_scores_knn), min(val_scores_knn)) - 0.01, 1.0)\n",
    "    for spine in plt.gca().spines.values():\n",
    "        spine.set_color('lightgray')\n",
    "        spine.set_linewidth(1.5)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e9b072-85d3-49e3-94cd-9d91ed9863b8",
   "metadata": {},
   "source": [
    "🔸 Training ROC-AUC decreases as the number of neighbors increases:\n",
    "\n",
    "This is expected. Larger n_neighbors values lead to smoother decision boundaries, meaning the model becomes less flexible and slightly underfits the training data.\n",
    "\n",
    "🔸 Validation ROC-AUC increases with larger n_neighbors:\n",
    "\n",
    "The validation performance improves steadily from n=5 to n=25.\n",
    "\n",
    "This indicates better generalization and reduced overfitting.\n",
    "\n",
    "🔸 The gap between Train and Validation scores narrows:\n",
    "\n",
    "At lower n values, the training score is much higher than validation, which is a sign of overfitting.\n",
    "\n",
    "As n increases, the model sacrifices a bit of training accuracy in favor of better performance on unseen data (validation), which is desirable.\n",
    "\n",
    "🔸 Conclusion:\n",
    "\n",
    "The best value for n_neighbors in this setup is 25, as it yields the highest validation ROC-AUC.\n",
    "\n",
    "The decreasing training performance alongside increasing validation performance suggests the model is moving away from overfitting and achieving better generalization.\n",
    "\n",
    "This is a healthy trade-off between bias and variance in KNN models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b73191-3fa9-4993-b322-2ea4ac16fd77",
   "metadata": {},
   "source": [
    "### <b> Final Training and Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d931a9ba-ad94-4971-b229-25fdccd4259f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_knn, x_test_knn, y_train_knn, y_test_knn = train_test_split(x, y, test_size=best_test_size_knn, random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafe710f-70e5-4fc7-ac61-3014899efdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the best hyperparameters obtained from RandomizedSearchCV\n",
    "best_params_knn = {'algorithm': 'brute', 'leaf_size': 24, 'metric': 'manhattan',\n",
    "               'n_neighbors': 25, 'p': 2, 'weights': 'uniform'}\n",
    "\n",
    "# Build the KNN model using the best parameters\n",
    "best_knn_model = KNeighborsClassifier(**best_params_knn)\n",
    "best_knn_model.fit(x_train_knn, y_train_knn)\n",
    "\n",
    "print(\"Best Parameters:\", grid.best_params_)\n",
    "\n",
    "# Predict the probability of the positive class for test data\n",
    "y_score_knn = best_knn_model.predict_proba(x_test_knn)[:, 1]\n",
    "\n",
    "# Calculate ROC Curve and AUC score\n",
    "fpr, tpr, _ = roc_curve(y_test_knn, y_score_knn)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Calculate Precision-Recall curve and Average Precision score\n",
    "precision, recall, _ = precision_recall_curve(y_test_knn, y_score_knn)\n",
    "avg_prec = average_precision_score(y_test_knn, y_score_knn)\n",
    "\n",
    "# Plot ROC and Precision-Recall curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5), dpi=200)\n",
    "\n",
    "# ROC Curve\n",
    "axes[0].plot(fpr, tpr, label=f'ROC AUC = {roc_auc:.2f}', color='#90BE6D')\n",
    "axes[0].plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "axes[0].set_xlabel('False Positive Rate', fontsize=10)\n",
    "axes[0].set_ylabel('True Positive Rate', fontsize=10)\n",
    "axes[0].set_title('ROC Curve', fontsize=12, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Precision-Recall Curve\n",
    "axes[1].plot(recall, precision, label=f'AP = {avg_prec:.2f}', color='#EA9010')\n",
    "axes[1].set_xlabel('Recall', fontsize=10)\n",
    "axes[1].set_ylabel('Precision', fontsize=10)\n",
    "axes[1].set_title('Precision-Recall Curve', fontsize=12, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "for ax in axes:\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_edgecolor('#D3D3D3') \n",
    "        spine.set_linewidth(1.5)\n",
    "\n",
    "plt.suptitle('Performance Analysis of Tuned KNN Model', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc49983-853b-4cbc-8fd6-53550bdea029",
   "metadata": {},
   "source": [
    "🔶 ROC Curve\n",
    "\n",
    "ROC AUC = 0.96\n",
    "\n",
    "This is an excellent score, indicating the classifier has a very strong ability to distinguish between the positive and negative classes.\n",
    "\n",
    "The curve is well above the diagonal (random chance line), which is a good sign.\n",
    "\n",
    "A True Positive Rate (TPR) near 1 with a low False Positive Rate (FPR) at many thresholds suggests low Type II error and good sensitivity.\n",
    "\n",
    "🔶 Precision-Recall Curve\n",
    "\n",
    "Average Precision (AP) = 0.80\n",
    "\n",
    "This is also a solid result, especially for imbalanced datasets (which PR curves are particularly helpful for).\n",
    "\n",
    "The curve starts high (indicating strong precision at low recall levels), but as recall increases, precision gradually decreases — a typical trade-off.\n",
    "\n",
    "This suggests the model maintains good performance but precision drops when trying to capture more positives (i.e., higher recall).\n",
    "\n",
    "🔶 Overall Interpretation\n",
    "\n",
    "High ROC AUC (0.96) + Good AP (0.80) indicate that the model performs very well in both:\n",
    "\n",
    "Ranking ability (ROC)\n",
    "\n",
    "Precision-recall balance, especially under class imbalance scenarios.\n",
    "\n",
    "This model seems well-calibrated and generalizes effectively — likely a result of good hyperparameter tuning and cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d383903f-b2be-4781-bbe5-9db85d0ffc0b",
   "metadata": {},
   "source": [
    "### <b>Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488833f4-f772-4dc6-8d1f-43095716bcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict final class labels on the test data\n",
    "y_pred_knn = best_knn_model.predict(x_test_knn)\n",
    "\n",
    "# محاسبه و نمایش متریک‌های ارزیابی\n",
    "print(\"🔶 Accuracy:\", accuracy_score(y_test_knn, y_pred_knn))\n",
    "print(\"🔶 Precision:\", precision_score(y_test_knn, y_pred_knn, average='weighted'))\n",
    "print(\"🔶 Recall:\", recall_score(y_test_knn, y_pred_knn, average='weighted'))\n",
    "print(\"🔶 F1 Score:\", f1_score(y_test_knn, y_pred_knn, average='weighted'))\n",
    "print(\"\\n🔶 Classification Report:\\n\", classification_report(y_test_knn, y_pred_knn))\n",
    "\n",
    "# ماتریس آشفتگی (Confusion Matrix)\n",
    "cm_knn = confusion_matrix(y_test_knn, y_pred_knn)\n",
    "fig, ax = plt.subplots(figsize=(4, 4), dpi=150)\n",
    "\n",
    "sns.heatmap(cm_knn, annot=True, fmt='d', cmap='Greens', cbar=False, square=True,\n",
    "            annot_kws={\"size\": 10}, ax=ax)\n",
    "\n",
    "ax.set_xlabel(\"Predicted Label\", fontsize=10)\n",
    "ax.set_ylabel(\"True Label\", fontsize=10)\n",
    "ax.set_title(\"Confusion Matrix\", fontsize=12, fontweight='bold')\n",
    "ax.tick_params(axis='both', labelsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948f8132-ed95-4173-9da9-38c0bb99eb8b",
   "metadata": {},
   "source": [
    "🔶 Overall Metrics \n",
    "\n",
    "🔶 Accuracy: 0.923 \n",
    "\n",
    "→ Very high overall, but accuracy alone can be misleading in imbalanced datasets.\n",
    "\n",
    "🔶 Precision: 0.928 \n",
    "\n",
    "→ The model is precise when predicting the positive class (few false positives).\n",
    "\n",
    "🔶 Recall: 0.923 \n",
    "\n",
    "→ Indicates the model identifies most true instances overall — but let’s break it down class-wise.\n",
    "\n",
    "🔶 F1 Score: 0.908 \n",
    "\n",
    "→ Strong harmonic mean of precision and recall.\n",
    "\n",
    "🔶 The model perfectly identifies all class 0 instances (True Negatives), but fails to identify class 1 instances (positives).\n",
    "\n",
    "🔶 Recall for class 1 is only 6%, meaning the model is missing 94% of true positive cases \n",
    "\n",
    "🔶 Precision for class 1 is technically perfect (1.00), but that’s because it only predicted 10 instances as class 1 — and they all happened to be correct. This is a side effect of extreme underprediction.\n",
    "\n",
    "🔶his confirms the model is biased toward the majority class (class 0). It barely detects class 1, despite having relatively high overall accuracy — a classic case of performance being inflated due to class imbalance.\n",
    "\n",
    "🔶 Conclusion:\n",
    "\n",
    "Accuracy is misleading here — the model performs poorly on the minority class.\n",
    "\n",
    "Although ROC AUC (from the previous image) is 0.96, the classification threshold likely needs to be adjusted to improve recall for class 1.\n",
    "Consider:\n",
    "\n",
    "Adjusting the decision threshold\n",
    "\n",
    "Using resampling techniques (e.g., SMOTE)\n",
    "\n",
    "Applying class weighting or focal loss\n",
    "\n",
    "Evaluating cost-sensitive metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d86898-c379-4130-9e80-6f6834659419",
   "metadata": {},
   "source": [
    "### <b> Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a11985-5123-4292-9555-45266e26c4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute feature importance using Permutation Importance\n",
    "result = permutation_importance(best_knn_model, x_test_knn, y_test_knn, n_repeats=10, random_state=0)\n",
    "\n",
    "# Create a DataFrame from the results\n",
    "perm_df = pd.DataFrame({\n",
    "    'Feature': x.columns,\n",
    "    'Importance': result.importances_mean,\n",
    "    'Std': result.importances_std\n",
    "})\n",
    "# Add a column with absolute importance values and sort\n",
    "perm_df['AbsImportance'] = np.abs(perm_df['Importance'])\n",
    "perm_df.sort_values('AbsImportance', ascending=False, inplace=True)\n",
    "\n",
    "# Plot the top 10 most important features\n",
    "plt.figure(figsize=(10, 4), dpi=200)\n",
    "sns.barplot(x='Importance', y='Feature', data=perm_df.head(10), palette=\"YlGn\")\n",
    "plt.title('Top 10 Important Features (KNN)', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Customize plot appearance\n",
    "for spine in plt.gca().spines.values():\n",
    "    spine.set_edgecolor('#D3D3D3') \n",
    "    spine.set_linewidth(1.5)\n",
    "\n",
    "plt.xlabel('Importance', fontsize=10)\n",
    "plt.ylabel('Feature', fontsize=10)\n",
    "plt.tick_params(axis='x', labelsize=8)\n",
    "plt.tick_params(axis='y', labelsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09602170-1e89-4523-b727-8fcf4ee4e3e1",
   "metadata": {},
   "source": [
    "🔶 Income and CCAvg_Annual (average annual credit card usage) are by far the most influential features in the KNN model. This indicates that financial status plays a critical role in prediction.\n",
    "\n",
    "🔶 Demographic features such as Age, Experience, and Geographic features (lat, lng) are moderately important but much less impactful than financial indicators.\n",
    "\n",
    "🔶 Family composition (Family_4.0, Family_1.0) and education level (Education_2.0) have minimal importance, showing low sensitivity in the model.\n",
    "\n",
    "🔶 Interestingly, the CD Account feature (indicating whether a customer has a Certificate of Deposit account) has slight but positive importance, suggesting a small contribution to the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c624bc3d-fb1f-4176-82ce-26c88338714f",
   "metadata": {},
   "source": [
    "## <b> Complement Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e70fb2-d9cc-49b3-a6a2-2883cdcb969f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into features (X) and target (y)\n",
    "x = merged_df.drop('Personal Loan', axis=1)\n",
    "y = merged_df['Personal Loan']\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cec9a8-853d-4adb-8ca5-9d1d17484b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Defining the feature lists for different types of data\n",
    "normal_features = ['Age', 'Experience']\n",
    "skewed_features = ['Income', 'CCAvg_Annual', 'Mortgage']\n",
    "categorical_features = ['Family', 'Education']\n",
    "binary_features = ['Securities Account', 'CD Account', 'Online', 'CreditCard']\n",
    "geo_features = ['lat', 'lng']\n",
    "\n",
    "# 2. Defining transformers\n",
    "# Transformer for features with normal distribution, applying MinMax scaling\n",
    "minmax = MinMaxScaler()\n",
    "\n",
    "# Transformer for skewed features, applying logarithmic transformation followed by MinMax scaling\n",
    "log_and_minmax = Pipeline(steps=[\n",
    "    ('log', FunctionTransformer(func=lambda x: np.log1p(x))), # Applying log transformation\n",
    "    ('minmax', MinMaxScaler()) # Scaling the transformed data\n",
    "])\n",
    "\n",
    "# OneHotEncoder for categorical features\n",
    "categorical = OneHotEncoder(drop=None, sparse_output=False)\n",
    "\n",
    "# 3. Defining the ColumnTransformer to apply different transformations to different feature sets\n",
    "preprocessor_cnb = ColumnTransformer(transformers=[\n",
    "    ('normals', minmax, normal_features), # Apply MinMax scaling to normal features\n",
    "    ('skewed', log_and_minmax, skewed_features), # Apply log transformation + MinMax scaling to skewed features\n",
    "    ('categorical', categorical, categorical_features), # Apply OneHot encoding to categorical features\n",
    "    ('geo', minmax, geo_features), # Apply MinMax scaling to geographical features\n",
    "    ('passthrough', 'passthrough', binary_features) # Leave binary features unchanged (no transformation)\n",
    "])\n",
    "\n",
    "# 4. Applying preprocessing to the dataset\n",
    "# Make a copy of the input dataset to avoid modifying the original dataset\n",
    "x_cnb = x.copy(deep=True)\n",
    "# Apply the defined transformations to the data\n",
    "X_cnb_preprocessed = preprocessor_cnb.fit_transform(x_cnb)\n",
    "\n",
    "# Save the preprocessor for later use\n",
    "scaler_cnb = preprocessor_cnb\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Create final DataFrame with column names\n",
    "# ---------------------------\n",
    "\n",
    "# Gather all output feature names including those from OneHotEncoder\n",
    "output_columns = (\n",
    "    normal_features +\n",
    "    skewed_features +\n",
    "    list(preprocessor.named_transformers_['categorical'].get_feature_names_out(categorical_features)) +\n",
    "    geo_features +\n",
    "    binary_features\n",
    ")\n",
    "\n",
    "# Create a new DataFrame with transformed features and target column\n",
    "final_df2 = pd.DataFrame(X_cnb_preprocessed, columns=output_columns)\n",
    "final_df2['Personal Loan'] = y.values\n",
    "\n",
    "print(final_df2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ca424f-d003-4885-9b06-af7614fffe69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the dataset into features (X) and target variable (y)\n",
    "x = final_df2.drop('Personal Loan', axis=1)\n",
    "y = final_df2['Personal Loan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a358f274-7747-42b5-9983-99440a2bd13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to store evaluation results\n",
    "test_sizes_cnb = []\n",
    "test_aucs_cnb = []\n",
    "train_aucs_cnb = []\n",
    "\n",
    "for i in test_size_list:\n",
    "    x_train_cnb, x_test_cnb, y_train_cnb, y_test_cnb = train_test_split(\n",
    "        x, y, test_size=i, random_state=0, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Initialize and train the Complement Naive Bayes model\n",
    "    cnb_model = ComplementNB()\n",
    "    cnb_model.fit(x_train_cnb, y_train_cnb)\n",
    "\n",
    "    # Predict class probabilities for training and test sets\n",
    "    train_proba_cnb = cnb_model.predict_proba(x_train_cnb)[:, 1]\n",
    "    test_proba_cnb = cnb_model.predict_proba(x_test_cnb)[:, 1]\n",
    "\n",
    "    # Calculate ROC-AUC scores for training and testing\n",
    "    train_auc_cnb = roc_auc_score(y_train_cnb, train_proba_cnb)\n",
    "    test_auc_cnb = roc_auc_score(y_test_cnb, test_proba_cnb)\n",
    "\n",
    "    # Store results\n",
    "    test_sizes_cnb.append(i)\n",
    "    train_aucs_cnb.append(train_auc_cnb)\n",
    "    test_aucs_cnb.append(test_auc_cnb)\n",
    "\n",
    "    # Predict class labels on the test set\n",
    "    y_pred_cnb = cnb_model.predict(x_test_cnb)\n",
    "\n",
    "    print(f\"\\n🔶 Test size: {i}\")\n",
    "    print(confusion_matrix(y_test_cnb, y_pred_cnb))\n",
    "    print(classification_report(y_test_cnb, y_pred_cnb))\n",
    "    print(f\"ROC-AUC Score: {test_auc_cnb:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fed888-dad6-47e5-b166-97322974e105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best test size based on the highest ROC-AUC score on the test set\n",
    "best_index_cnb = np.argmax(test_aucs_cnb)\n",
    "best_test_size_cnb = test_sizes_cnb[best_index_cnb]\n",
    "best_test_auc_cnb = test_aucs_cnb[best_index_cnb]\n",
    "\n",
    "# Plot the ROC-AUC scores for train and test sets across different test sizes\n",
    "plt.figure(figsize=(10, 4), dpi=200)\n",
    "plt.plot(test_sizes_cnb, train_aucs_cnb, marker='o', label='Train ROC-AUC', color='#90BE6D') # Plot training ROC-AUC\n",
    "plt.plot(test_sizes_cnb, test_aucs_cnb, marker='s', label='Test ROC-AUC', color='#EA9010') # Plot test ROC-AUC\n",
    "\n",
    "plt.annotate(\n",
    "    f'Best Test Size = {best_test_size_cnb}',\n",
    "    xy=(best_test_size_cnb, best_test_auc_cnb),\n",
    "    xytext=(best_test_size_cnb - 0.1, best_test_auc_cnb + 0.01),\n",
    "    arrowprops=dict(facecolor='blue', edgecolor='blue', arrowstyle='->', lw=1.5),\n",
    "    fontsize=10, color='blue', fontweight='bold'\n",
    ")\n",
    "\n",
    "# Set plot titles and labels\n",
    "plt.title('Train vs Test ROC-AUC Across Different Test Sizes for ComplementNB', fontsize=12, fontweight='bold')\n",
    "plt.xlabel('Test Size', fontsize=10)\n",
    "plt.ylabel('ROC-AUC Score', fontsize=10)\n",
    "\n",
    "plt.ylim(0.75, 0.85)\n",
    "plt.legend(fontsize=8)\n",
    "plt.grid(True)\n",
    "plt.xticks(fontsize=8)\n",
    "plt.yticks(fontsize=8)\n",
    "\n",
    "for spine in plt.gca().spines.values():\n",
    "    spine.set_color('lightgray')\n",
    "    spine.set_linewidth(1.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c0bb53-9a3e-4434-8002-e459eed999b7",
   "metadata": {},
   "source": [
    "🔶 Test Size = 0.1\n",
    "\n",
    "Train ROC-AUC ≈ 0.793\n",
    "\n",
    "Test ROC-AUC ≈ 0.778\n",
    "\n",
    "There is a small positive gap between the test and train scores. The model performs slightly better on the test set, which might suggest mild underfitting, but overall it indicates decent generalization.\n",
    "\n",
    "🔶 Test Size = 0.2\n",
    "\n",
    "Train ROC-AUC ≈ 0.794\n",
    "\n",
    "Test ROC-AUC ≈ 0.785\n",
    "\n",
    "The scores are very close, indicating a balanced performance. The model handles both train and test data consistently.\n",
    "\n",
    "🔶 Test Size = 0.3\n",
    "\n",
    "Train ROC-AUC ≈ 0.796\n",
    "\n",
    "Test ROC-AUC ≈ 0.775\n",
    "\n",
    "Slight improvement in training score, but test performance dips slightly. This could hint at mild overfitting, though still within an acceptable range.\n",
    "\n",
    "🔶 Test Size = 0.4\n",
    "\n",
    "Train ROC-AUC ≈ 0.798\n",
    "\n",
    "Test ROC-AUC ≈ 0.781\n",
    "\n",
    "The gap between train and test scores increases slightly, but the test performance improves. This suggests improved generalization, though with a bit more overfitting compared to previous sizes.\n",
    "\n",
    "⭐ Test Size = 0.5 ←(Best Point)\n",
    "\n",
    "Train ROC-AUC ≈ 0.801\n",
    "\n",
    "Test ROC-AUC ≈ 0.798\n",
    "\n",
    "This is the best point based on the ROC-AUC for the test set. The small gap between train and test indicates excellent generalization. The model achieves its peak test performance here.\n",
    "\n",
    "🔶 Conclusion:\n",
    "\n",
    "The best test size is 0.5, where the Test ROC-AUC reaches its highest value (~0.798).\n",
    "\n",
    "The model generalizes best at this point, with only a slight increase in training performance.\n",
    "\n",
    "Although smaller test sizes (like 0.1 or 0.2) offer stability, the 0.5 test split provides the most balanced and optimal model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96abaf1b-2baf-4fda-bdcf-98b58d1795ea",
   "metadata": {},
   "source": [
    "### <b> Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df729d7a-f914-4fc5-9fe5-0886d6a493ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameter grid for Complement Naive Bayes\n",
    "param_grid_cnb = {\n",
    "    'alpha': [0.01, 0.1, 0.5, 1.0, 2.0, 5.0, 10.0],\n",
    "    'fit_prior': [True, False], # Whether to learn class prior probabilities or not\n",
    "    'norm': [True, False]       # Whether to normalize feature values\n",
    "}\n",
    "\n",
    "# List to store the results for each CV value\n",
    "cv_results_cnb = []\n",
    "\n",
    "# Initialize the model\n",
    "cnb_model = ComplementNB()\n",
    "\n",
    "# Perform grid search over different cross-validation values\n",
    "for cv in cv_values:\n",
    "    print(f\"\\n🔶 Running GridSearchCV for ComplementNB with cv={cv}\")\n",
    "\n",
    "    # Setup GridSearchCV with Stratified K-Fold\n",
    "    grid = GridSearchCV(\n",
    "        estimator=cnb_model,\n",
    "        param_grid=param_grid_cnb,\n",
    "        cv=StratifiedKFold(n_splits=cv, shuffle=True, random_state=0),\n",
    "        scoring='roc_auc',\n",
    "        return_train_score=True,\n",
    "        n_jobs=-1,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Fit the model to data\n",
    "    grid.fit(x, y)\n",
    "    \n",
    "    # Extract the best training and validation scores\n",
    "    best_idx_cnb = grid.best_index_\n",
    "    train_auc_cnb = grid.cv_results_['mean_train_score'][best_idx_cnb]\n",
    "    val_auc_cnb = grid.cv_results_['mean_test_score'][best_idx_cnb]\n",
    "\n",
    "    # Store the results\n",
    "    cv_results_cnb.append({\n",
    "        'cv': cv,\n",
    "        'grid': grid,\n",
    "        'train_auc': train_auc_cnb,\n",
    "        'val_auc': val_auc_cnb\n",
    "    })\n",
    "\n",
    "    print(f\" cv={cv} ➤ Train ROC-AUC: {train_auc_cnb:.4f}, Validation ROC-AUC: {val_auc_cnb:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9e7035-d403-4c77-8add-82d46b5a76ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# پیدا کردن بهترین مدل بر اساس بیشترین AUC اعتبارسنجی\n",
    "best_entry = max(cv_results_cnb, key=lambda x: x['val_auc'])\n",
    "best_cv_cnb = best_entry['cv']\n",
    "best_model_cnb = best_entry['grid']\n",
    "best_score_cnb = best_entry['val_auc']\n",
    "\n",
    "# نمایش نتایج\n",
    "print(\"\\n🔶 Best number of folds (cv):\", best_cv_cnb)\n",
    "print(\"🔶 best combination of hyperparameters:\", best_model_cnb.best_params_)\n",
    "print(\"🔶 Highest ROC-AUC score:\", round(best_score_cnb, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eff99d8-1e92-487a-b3f0-74cf45879afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data from CV results\n",
    "cv_vals_cnb = [r['cv'] for r in cv_results_cnb]                   # List of CV values\n",
    "train_auc_scores_cnb = [r['train_auc'] for r in cv_results_cnb]   # Corresponding train AUCs\n",
    "val_auc_scores_cnb = [r['val_auc'] for r in cv_results_cnb]       # Corresponding validation AUCs\n",
    "\n",
    "# Identify the best CV value based on highest validation AUC\n",
    "best_idx_cnb = np.argmax(val_auc_scores_cnb)\n",
    "best_cv_cnb = cv_vals_cnb[best_idx_cnb]\n",
    "best_val_auc_cnb = val_auc_scores_cnb[best_idx_cnb]\n",
    "\n",
    "# Plotting the train and validation ROC-AUC scores across CV folds\n",
    "plt.figure(figsize=(10, 4), dpi=200)\n",
    "plt.plot(cv_vals_cnb, train_auc_scores_cnb, marker='o', label='Train ROC-AUC', color='#90BE6D')\n",
    "plt.plot(cv_vals_cnb, val_auc_scores_cnb, marker='s', label='Validation ROC-AUC', color='#EA9010')\n",
    "\n",
    "# Chart labels and title\n",
    "plt.xlabel('CV folds (k)', fontsize=10)\n",
    "plt.ylabel('ROC-AUC Score', fontsize=10)\n",
    "plt.title('Train vs Validation ROC-AUC Across Different CV Values for ComplementNB', fontsize=12, fontweight='bold')\n",
    "plt.ylim(min(min(train_auc_scores_cnb), min(val_auc_scores_cnb)) - 0.01, 0.9)\n",
    "plt.xticks(cv_vals_cnb)\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# Annotate best CV\n",
    "plt.annotate(\n",
    "    f'Best CV = {best_cv_cnb}', \n",
    "    xy=(best_cv_cnb, best_val_auc_cnb), \n",
    "    xytext=(best_cv_cnb - 0.8, best_val_auc_cnb + 0.05), \n",
    "    arrowprops=dict(facecolor='blue', edgecolor='blue', arrowstyle='->', lw=1.5),\n",
    "    fontsize=10, color='blue', fontweight='bold'\n",
    ")\n",
    "\n",
    "plt.xticks(fontsize=8)\n",
    "plt.yticks(fontsize=8)\n",
    "for spine in plt.gca().spines.values():\n",
    "    spine.set_color('lightgray')\n",
    "    spine.set_linewidth(1.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216b6f3b-0330-4530-9ae1-8760dfa889ce",
   "metadata": {},
   "source": [
    "🔶 CV = 3\n",
    "\n",
    "Train ROC-AUC ≈ 0.793\n",
    "\n",
    "Validation ROC-AUC ≈ 0.782\n",
    "\n",
    "A small gap exists between train and validation. Validation score is slightly lower, indicating the model generalizes reasonably well, though may slightly underfit with fewer folds.\n",
    "\n",
    "🔶 CV = 5\n",
    "\n",
    "Train ROC-AUC ≈ 0.793\n",
    "\n",
    "Validation ROC-AUC ≈ 0.784\n",
    "\n",
    "Performance on the validation set improves slightly. Still a balanced outcome, and the results remain stable.\n",
    "\n",
    "🔶 CV = 7\n",
    "\n",
    "Train ROC-AUC ≈ 0.793\n",
    "\n",
    "Validation ROC-AUC ≈ 0.785\n",
    "\n",
    "The scores remain consistent. Increasing k improves validation performance very slightly. The model maintains stability without noticeable overfitting.\n",
    "\n",
    "⭐ CV = 10 ←(Best Point)\n",
    "\n",
    "Train ROC-AUC ≈ 0.793\n",
    "\n",
    "Validation ROC-AUC ≈ 0.786\n",
    "\n",
    "This is the highest Validation ROC-AUC among all CV folds. The gap between train and validation scores is minimal, indicating excellent generalization and reliable performance.\n",
    "\n",
    "🔶 Conclusion:\n",
    "\n",
    "The best CV fold is k = 10, where the Validation ROC-AUC peaks at ~0.786.\n",
    "\n",
    "Across all values of k, both Train and Validation scores are remarkably stable, which shows that the model is robust and not highly sensitive to the number of CV folds.\n",
    "\n",
    "Using more folds (like k=10) slightly improves validation performance while maintaining model stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba95504-243e-4de0-8b8f-b53490b04b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the GridSearchCV object for CV=10\n",
    "grid = cv_results_cnb[cv_vals_cnb.index(10)]['grid']\n",
    "results_cnb = grid.cv_results_\n",
    "\n",
    "# Filter entries where 'norm' and 'fit_prior' are both True\n",
    "filtered_idxs_cnb = [\n",
    "    i for i, params in enumerate(results_cnb['params'])\n",
    "    if params.get('norm') == True and\n",
    "       params.get('fit_prior') == True\n",
    "]\n",
    "\n",
    "# Extract alpha values and corresponding mean train/validation ROC-AUC scores\n",
    "alpha_vals_cnb = [results_cnb['params'][i]['alpha'] for i in filtered_idxs_cnb]\n",
    "val_scores_cnb = [results_cnb['mean_test_score'][i] for i in filtered_idxs_cnb]\n",
    "train_scores_cnb = [results_cnb['mean_train_score'][i] for i in filtered_idxs_cnb]\n",
    "\n",
    "# Identify the alpha value that yields the highest validation ROC-AUC\n",
    "best_idx_cnb = np.argmax(val_scores_cnb)\n",
    "best_alpha_cnb = alpha_vals_cnb[best_idx_cnb]\n",
    "best_val_score_cnb = val_scores_cnb[best_idx_cnb]\n",
    "\n",
    "# Plot train and validation ROC-AUC scores across different alpha values\n",
    "plt.figure(figsize=(10, 4), dpi=200)\n",
    "plt.plot(alpha_vals_cnb, train_scores_cnb, marker='o', label='Train ROC-AUC', color='#90BE6D')\n",
    "plt.plot(alpha_vals_cnb, val_scores_cnb, marker='s', label='Validation ROC-AUC', color='#EA9010')\n",
    "\n",
    "# Use logarithmic scale for alpha axis (since alpha spans wide values)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Alpha (log scale)', fontsize=10)\n",
    "plt.ylabel('ROC-AUC Score', fontsize=10)\n",
    "plt.title('Train vs Validation ROC-AUC Across Different Alpha Values\\n(CV=10, Fixed Params)', fontsize=12, fontweight='bold')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# Annotate best alpha\n",
    "plt.annotate(\n",
    "    f'Best α = {best_alpha_cnb}',\n",
    "    xy=(best_alpha_cnb, best_val_score_cnb),\n",
    "    xytext=(best_alpha_cnb, best_val_score_cnb + 0.02),\n",
    "    arrowprops=dict(facecolor='blue', edgecolor='blue', arrowstyle='->', lw=1.5),\n",
    "    fontsize=10, color='blue', fontweight='bold'\n",
    ")\n",
    "\n",
    "plt.ylim(min(min(train_scores_cnb), min(val_scores_cnb)) - 0.01, 0.9)\n",
    "\n",
    "for spine in plt.gca().spines.values():\n",
    "    spine.set_color('lightgray')\n",
    "    spine.set_linewidth(1.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a912b3-1265-43e4-a191-a2fab4707749",
   "metadata": {},
   "source": [
    "⭐ α = 0.01 ←(Best Point)\n",
    "\n",
    "Train ROC-AUC ≈ 0.792\n",
    "\n",
    "Validation ROC-AUC ≈ 0.782\n",
    "\n",
    "This is the best-performing model across the tested values.\n",
    "\n",
    "Minimal gap between training and validation suggests low variance and good generalization.\n",
    "\n",
    "🔸 α = 0.1 to 10 (rightward on log scale)\n",
    "\n",
    "Gradual decline in both training and validation ROC-AUC scores.\n",
    "\n",
    "As α increases, the model gets more regularized (i.e., coefficients are penalized more).\n",
    "\n",
    "This leads to underfitting, especially at α = 10 (lowest scores).\n",
    "\n",
    "The validation score drops more sharply than training as α increases → reduced model flexibility.\n",
    "\n",
    "🔸 Trend Observed\n",
    "\n",
    "Train score is consistently slightly higher than validation, which is normal.\n",
    "\n",
    "The gap between curves remains small → no major signs of overfitting.\n",
    "\n",
    "However, higher α values harm performance due to excessive regularization.\n",
    "\n",
    "🔸 Conclusion:\n",
    "\n",
    "The model achieves its optimal balance at α = 0.01, where both training and validation ROC-AUC scores are highest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e91a9bb-7192-44a4-84e3-db4a55b1ed27",
   "metadata": {},
   "source": [
    "### <b> Final Training and Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d476476b-de19-4741-85c3-a3065fbe0244",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_cnb, x_test_cnb, y_train_cnb, y_test_cnb = train_test_split(x, y, test_size=best_test_size_cnb, random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3452dd6c-41db-4c3f-ae94-4ef71ce3ccac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume 'grid' contains the GridSearchCV results for ComplementNB\n",
    "best_cnb_model = grid.best_estimator_  # Best model from GridSearchCV\n",
    "print(\"Best Parameters:\", grid.best_params_)\n",
    "\n",
    "# Predict probabilities for the positive class (class 1)\n",
    "y_score_cnb = best_cnb_model.predict_proba(x_test_cnb)[:, 1]\n",
    "\n",
    "# Compute ROC curve metrics\n",
    "fpr, tpr, _ = roc_curve(y_test_cnb, y_score_cnb)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Compute Precision-Recall curve metrics\n",
    "precision, recall, _ = precision_recall_curve(y_test_cnb, y_score_cnb)\n",
    "avg_prec = average_precision_score(y_test_cnb, y_score_cnb)\n",
    "\n",
    "# Plotting ROC and Precision-Recall curves side by side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5), dpi=200)\n",
    "\n",
    "# ROC Curve\n",
    "axes[0].plot(fpr, tpr, label=f'ROC AUC = {roc_auc:.2f}', color='#90BE6D')\n",
    "axes[0].plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "axes[0].set_xlabel('False Positive Rate', fontsize=10)\n",
    "axes[0].set_ylabel('True Positive Rate', fontsize=10)\n",
    "axes[0].set_title('ROC Curve', fontsize=12, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Precision-Recall Curve\n",
    "axes[1].plot(recall, precision, label=f'AP = {avg_prec:.2f}', color='#EA9010')\n",
    "axes[1].set_xlabel('Recall', fontsize=10)\n",
    "axes[1].set_ylabel('Precision', fontsize=10)\n",
    "axes[1].set_title('Precision-Recall Curve', fontsize=12, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "for ax in axes:\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_edgecolor('#D3D3D3')\n",
    "        spine.set_linewidth(1.5)\n",
    "\n",
    "plt.suptitle('Performance Analysis of Tuned Complement Naive Bayes Model', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e18596-3ec8-4e86-af38-e22da1b8cb75",
   "metadata": {},
   "source": [
    "🔸ROC Curve\n",
    "\n",
    "ROC AUC = 0.80\n",
    "\n",
    "This means the model has moderate discriminatory power. AUC of 0.80 indicates that the classifier is able to distinguish between the positive and negative classes reasonably well, but it's not highly strong.\n",
    "\n",
    "The curve rises above the diagonal (random guess), but not steeply.\n",
    "\n",
    "Still, there's room for improvement — especially compared to models with ROC AUC ≥ 0.90.\n",
    "\n",
    "🔸 Precision-Recall Curve\n",
    "\n",
    "Average Precision (AP) = 0.39\n",
    "\n",
    "This is relatively low, particularly in imbalanced datasets where precision-recall metrics are more informative than ROC.\n",
    "\n",
    "The precision drops significantly as recall increases, indicating the model struggles to maintain high precision when trying to retrieve more positive cases.\n",
    "\n",
    "This suggests a high false positive rate when recall is prioritized.\n",
    "\n",
    "🔸 Conclusion\n",
    "The Complement Naive Bayes model performs moderately well in terms of ROC AUC, but poorly in terms of precision-recall.\n",
    "\n",
    "This could indicate the model is not well suited for tasks(like this) where positive class detection is critical or when the dataset is imbalanced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0667d65d-8b0f-4459-90e9-df471d24f837",
   "metadata": {},
   "source": [
    "### <b>Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed12dc0f-4b68-490e-8636-121b94bac28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict class labels using the best ComplementNB model\n",
    "y_pred_cnb = best_cnb_model.predict(x_test_cnb)\n",
    "\n",
    "# Compute and display evaluation metrics\n",
    "print(\"🔶 Accuracy:\", accuracy_score(y_test_cnb, y_pred_cnb))\n",
    "print(\"🔶 Precision:\", precision_score(y_test_cnb, y_pred_cnb, average='weighted'))\n",
    "print(\"🔶 Recall:\", recall_score(y_test_cnb, y_pred_cnb, average='weighted'))\n",
    "print(\"🔶 F1 Score:\", f1_score(y_test_cnb, y_pred_cnb, average='weighted'))\n",
    "print(\"\\n🔶 Classification Report:\\n\", classification_report(y_test_cnb, y_pred_cnb))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_cnb = confusion_matrix(y_test_cnb, y_pred_cnb)\n",
    "# Plot the confusion matrix as a heatmap\n",
    "fig, ax = plt.subplots(figsize=(4, 4), dpi=150)\n",
    "\n",
    "sns.heatmap(cm_cnb, annot=True, fmt='d', cmap='Greens', cbar=False, square=True,\n",
    "            annot_kws={\"size\": 10}, ax=ax)\n",
    "\n",
    "ax.set_xlabel(\"Predicted Label\", fontsize=10)\n",
    "ax.set_ylabel(\"True Label\", fontsize=10)\n",
    "ax.set_title(\"Confusion Matrix\", fontsize=12, fontweight='bold')\n",
    "ax.tick_params(axis='both', labelsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92d8a37-edfc-4a3b-b7fc-577dbb2d1006",
   "metadata": {},
   "source": [
    "🔶 Overall Metrics:\n",
    "\n",
    "🔶 Accuracy: 0.6666 \n",
    "\n",
    "→ About 66.7% of the total predictions were correct.\n",
    "\n",
    "🔶 Precision: 0.9028 \n",
    "\n",
    "→ High precision means that when the model predicts class 1, it's usually correct.\n",
    "\n",
    "🔶 Recall: 0.6666\n",
    "\n",
    "→ The model retrieved about 66.6% of the actual positive cases.\n",
    "\n",
    "🔶 F1 Score: 0.7411 \n",
    "\n",
    "→ Harmonic mean of precision and recall.\n",
    "\n",
    "🔶 For class 0, the model performs well overall, especially in precision (0.97).\n",
    "\n",
    "🔶 For class 1, performance is very poor in precision (0.17), meaning a lot of false positives (samples predicted as 1 but are actually 0).\n",
    "\n",
    "🔶 Conclusion:\n",
    "\n",
    "The model favors the majority class (class 0) heavily.\n",
    "\n",
    "Performance for minority class (class 1) is poor, especially in precision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64e527f-4fba-454f-918a-d833fa5a2636",
   "metadata": {},
   "source": [
    "### <b> Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e35575-e68d-4e11-bc76-fc991aacb1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the feature names from the dataset\n",
    "features = x.columns\n",
    "\n",
    "# Compute the log-probability difference between class 1 and class 0\n",
    "# This difference is used as a measure of the relative importance of each feature\n",
    "log_prob_diff = best_cnb_model.feature_log_prob_[1] - best_cnb_model.feature_log_prob_[0]\n",
    "\n",
    "# Create a DataFrame to store feature names and their importance scores\n",
    "coeff_df = pd.DataFrame({'Feature': features, 'Importance': log_prob_diff})\n",
    "# Calculate the absolute importance to help with sorting\n",
    "coeff_df['AbsImportance'] = np.abs(coeff_df['Importance'])  \n",
    "# Sort the DataFrame by absolute importance in descending order\n",
    "coeff_df.sort_values('AbsImportance', ascending=False, inplace=True)\n",
    "\n",
    "# Generate a color palette with 10 greenish shades for the plot\n",
    "colors = sns.color_palette(\"YlGn\", n_colors=10)\n",
    "\n",
    "# Plot the top 10 most important features using a horizontal barplot\n",
    "plt.figure(figsize=(10, 4), dpi=200)\n",
    "sns.barplot(x='Importance', y='Feature', data=coeff_df.head(10), palette=colors)\n",
    "# Set the title of the plot\n",
    "plt.title('Top 10 Important Features (ComplementNB)', fontsize=12, fontweight='bold')\n",
    "\n",
    "for spine in plt.gca().spines.values():\n",
    "    spine.set_edgecolor('#D3D3D3')\n",
    "    spine.set_linewidth(1.5)\n",
    "\n",
    "plt.xlabel('Log Probability Difference', fontsize=10)\n",
    "plt.ylabel('Feature', fontsize=10)\n",
    "plt.tick_params(axis='x', labelsize=8)\n",
    "plt.tick_params(axis='y', labelsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62df1b3-60b3-4fa4-9d68-3b764db0b92d",
   "metadata": {},
   "source": [
    "🔶 CD Account is by far the most influential feature in the ComplementNB model. Its strong positive log-probability difference shows that having a Certificate of Deposit account is a key indicator of class 1. This feature contributes heavily to positive predictions.\n",
    "\n",
    "🔶 Education_1.0 and Mortgage both have strong negative influence, meaning they are more associated with class 0. This suggests that individuals with lower education levels or active mortgages are more likely to be predicted as class 0.\n",
    "\n",
    "🔶 Financial features like Income and CCAvg_Annual (average annual credit card spending) show moderate importance. These variables still influence the model’s output, particularly in favoring class 1, but less strongly than CD Account.\n",
    "\n",
    "🔶 Demographic and household structure features such as Family size (Family_1.0, Family_2.0, Family_3.0) and Education levels (Education_2.0, Education_3.0) show relatively minor contributions. Their log-probability differences are small, indicating low sensitivity in this model.\n",
    "\n",
    "🔶 Overall, the ComplementNB model leans heavily on a small set of high-impact financial features, with demographic details playing a supporting but limited role."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e87a265-249a-4464-a09c-356fcca77391",
   "metadata": {},
   "source": [
    "## <b> Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15663bc1-892c-4a8c-9a2e-0b0f7438d4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store the final evaluation results of the models\n",
    "results = []\n",
    "\n",
    "# Define a function to evaluate a model by calculating and storing various metrics\n",
    "def evaluate_model(name, model, X_train, X_test, y_train, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    # If the model supports probability estimates, get the probability for class 1\n",
    "    y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
    "\n",
    "    # Store the evaluation metrics in the results list\n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'Precision': precision_score(y_test, y_pred, average='weighted'),\n",
    "        'Recall': recall_score(y_test, y_pred, average='weighted'),\n",
    "        'F1 Score': f1_score(y_test, y_pred, average='weighted'),\n",
    "        'ROC AUC': roc_auc_score(y_test, y_proba) if y_proba is not None else 'N/A'\n",
    "    })\n",
    "\n",
    "# Assumption: Only KNN model has been trained so far, and we want to evaluate it\n",
    "evaluate_model(\"Logistic Regression\", best_log_reg_model, x_train, x_test, y_train, y_test)\n",
    "evaluate_model(\"KNN\", best_knn_model, x_train_knn, x_test_knn, y_train_knn, y_test_knn)\n",
    "evaluate_model(\"Complement Naive Bayes\", best_cnb_model, x_train_cnb, x_test_cnb, y_train_cnb, y_test_cnb)\n",
    "\n",
    "# Convert the results list to a DataFrame for tabular display\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print(\"🔶 Final Evaluation Metrics Table:\")\n",
    "display(results_df.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747bf366-7882-4381-98dd-cf0da49015e9",
   "metadata": {},
   "source": [
    "🔶 So, Logistic Regression is the best model among the three.\n",
    "    \n",
    "🔶 Although the KNN model has a higher accuracy than Logistic Regression, the ROC AUC score is a more appropriate evaluation metric.\n",
    "\n",
    "🔶 We are dealing with an imbalanced classification problem (90% negative class and 10% positive class), and:\n",
    "\n",
    "🔶 ROC AUC is a better choice because:\n",
    "\n",
    "<b>.</b> It is independent of the threshold.\n",
    "\n",
    "<b>.</b> It measures the model’s ability to correctly distinguish between classes.\n",
    "\n",
    "<b>.</b> Even if a model simply predicts only the majority class (resulting in high accuracy), its ROC AUC will still be low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acbd074-14a9-4379-a979-30d8af3b0b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a figure with two subplots side by side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "# Plot the first confusion matrix (e.g., Logistic Regression)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', cbar=False, ax=axes[0])\n",
    "axes[0].set_title('Confusion Matrix - Logistic Regression')\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('Actual')\n",
    "\n",
    "# Plot the second confusion matrix (e.g., KNN)\n",
    "sns.heatmap(cm_knn, annot=True, fmt='d', cmap='Greens', cbar=False, ax=axes[1])\n",
    "axes[1].set_title('Confusion Matrix - KNN')\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('Actual')\n",
    "\n",
    "# Adjust layout to prevent overlapping\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ef6f149e-8e19-4a37-91cf-93ca55be379f",
   "metadata": {},
   "source": [
    "🔶 However, by comparing the confusion matrices of all three models, we can say that a combination of Logistic Regression and KNN would yield the best possible outcome. Because our target in this problem is class 1 and in the logistic regression model 148 cases were correctly identified as class 1 while in the KNN model only 10 cases and a large number of errors were identified as class 0. On the other hand, in terms of cost, the KNN model is better because it does not make any mistakes in giving loan offers to those who do not accept the loan (TN=0) while in the logistic model 186 cases will be given loan offers incorrectly.\n",
    "\n",
    "🔶 But the Naive Bayes model does not perform well at all on this dataset, although the Naive Bayes complement can handle imbalanced data, but it still does not perform well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2269641-7db4-484f-9341-7298ef212b0d",
   "metadata": {},
   "source": [
    "## <b>Ensemble Model(Logistic Regression + KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f733c01a-85b4-4967-acf9-3eb1d87f674b",
   "metadata": {},
   "source": [
    "🔶 One way to build an ensemble model is VotingClassifier\n",
    "\n",
    "🔶 VotingClassifier is more useful for unbalanced data\n",
    "\n",
    "🔶 In VotingClassifier, because multiple models judge the data:\n",
    "\n",
    "The effect of noise or outliers is reduced.\n",
    "\n",
    "The prediction error is statistically reduced.\n",
    "\n",
    "🔶 In VotingClassifier, soft voting can be used (i.e., averaging the class probabilities). This makes:\n",
    "\n",
    "The outputs smoother and more accurate.\n",
    "\n",
    "Models that are more confident in their predictions have more weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d5e535-f0e2-4696-99d9-91c6fd9cb5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into features (X) and target (y)\n",
    "x = final_df.drop('Personal Loan', axis=1)\n",
    "y = final_df['Personal Loan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d474bb-306f-44c8-b502-5472e0888c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_log_reg_knn, x_test_log_reg_knn, y_train_log_reg_knn, y_test_log_reg_knn = train_test_split(x, y, test_size=0.4, random_state=0, stratify=y)\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('log_reg', best_log_reg_model), ('knn', best_knn_model)],\n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "voting_clf.fit(x_train_log_reg_knn, y_train_log_reg_knn)\n",
    "\n",
    "# Predict probabilities for the positive class (class 1)\n",
    "y_score_log_reg_knn = voting_clf.predict_proba(x_test_log_reg_knn)[:, 1]\n",
    "\n",
    "# Compute ROC curve metrics\n",
    "fpr, tpr, _ = roc_curve(y_test_log_reg_knn, y_score_log_reg_knn)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Compute Precision-Recall curve metrics\n",
    "precision, recall, _ = precision_recall_curve(y_test_log_reg_knn, y_score_log_reg_knn)\n",
    "avg_prec = average_precision_score(y_test_log_reg_knn, y_score_log_reg_knn)\n",
    "\n",
    "# Plotting ROC and Precision-Recall curves side by side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5), dpi=200)\n",
    "\n",
    "# ROC Curve\n",
    "axes[0].plot(fpr, tpr, label=f'ROC AUC = {roc_auc:.2f}', color='#90BE6D')\n",
    "axes[0].plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "axes[0].set_xlabel('False Positive Rate', fontsize=10)\n",
    "axes[0].set_ylabel('True Positive Rate', fontsize=10)\n",
    "axes[0].set_title('ROC Curve', fontsize=12, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Precision-Recall Curve\n",
    "axes[1].plot(recall, precision, label=f'AP = {avg_prec:.2f}', color='#EA9010')\n",
    "axes[1].set_xlabel('Recall', fontsize=10)\n",
    "axes[1].set_ylabel('Precision', fontsize=10)\n",
    "axes[1].set_title('Precision-Recall Curve', fontsize=12, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "for ax in axes:\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_edgecolor('#D3D3D3')\n",
    "        spine.set_linewidth(1.5)\n",
    "\n",
    "plt.suptitle('Performance Analysis of Tuned Ensemble Model', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73131ec-e469-4cfa-9778-5240ba469949",
   "metadata": {},
   "source": [
    "🔸 ROC Curve\n",
    "\n",
    "ROC AUC = 0.99\n",
    "\n",
    "This indicates excellent discriminatory power. AUC close to 1 means the model is highly capable of distinguishing between the positive and negative classes.\n",
    "\n",
    "The curve hugs the top-left corner, which is ideal, showing a very low false positive rate and high true positive rate.\n",
    "\n",
    "Such a steep and early rise in the ROC curve suggests the model consistently ranks positive instances higher than negatives.\n",
    "\n",
    "🔸 Precision-Recall Curve\n",
    "\n",
    "Average Precision (AP) = 0.92\n",
    "\n",
    "This is a very strong result, especially in scenarios where data is imbalanced(like this). Precision remains high even as recall increases, which suggests the model is effective at identifying positives without introducing many false positives.\n",
    "\n",
    "The curve only drops toward the end, indicating the model maintains high precision across most recall levels.\n",
    "\n",
    "This implies a low false positive rate, even when trying to capture a large portion of the true positives.\n",
    "\n",
    "🔸 Conclusion\n",
    "\n",
    "The Tuned Ensemble Model shows outstanding performance. Both ROC AUC and Average Precision are well above typical thresholds, indicating it is highly effective at both ranking and identifying positive cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d9704b-6ada-4b0f-9317-31fdb167dbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_log_reg_knn = voting_clf.predict(x_test_log_reg_knn)\n",
    "\n",
    "# Compute and display evaluation metrics\n",
    "print(\"🔶 Accuracy:\", accuracy_score(y_test_log_reg_knn, y_pred_log_reg_knn))\n",
    "print(\"🔶 Precision:\", precision_score(y_test_log_reg_knn, y_pred_log_reg_knn, average='weighted'))\n",
    "print(\"🔶 Recall:\", recall_score(y_test_log_reg_knn, y_pred_log_reg_knn, average='weighted'))\n",
    "print(\"🔶 F1 Score:\", f1_score(y_test_log_reg_knn, y_pred_log_reg_knn, average='weighted'))\n",
    "print(\"\\n🔶 Classification Report:\\n\", classification_report(y_test_log_reg_knn, y_pred_log_reg_knn))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm_log_reg_knn = confusion_matrix(y_test_log_reg_knn, y_pred_log_reg_knn)\n",
    "# Plot the confusion matrix as a heatmap\n",
    "fig, ax = plt.subplots(figsize=(4, 4), dpi=150)\n",
    "\n",
    "sns.heatmap(cm_log_reg_knn, annot=True, fmt='d', cmap='Greens', cbar=False, square=True,\n",
    "            annot_kws={\"size\": 10}, ax=ax)\n",
    "\n",
    "ax.set_xlabel(\"Predicted Label\", fontsize=10)\n",
    "ax.set_ylabel(\"True Label\", fontsize=10)\n",
    "ax.set_title(\"Confusion Matrix\", fontsize=12, fontweight='bold')\n",
    "ax.tick_params(axis='both', labelsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3033cc1-b7e0-4784-a6c7-771982f27a77",
   "metadata": {},
   "source": [
    "🔶 Overall Metrics:\n",
    "\n",
    "🔶 Accuracy: 0.9761\n",
    "\n",
    "→ About 97.6% of total predictions are correct — a very high overall accuracy.\n",
    "\n",
    "🔶 Precision: 0.9755\n",
    "\n",
    "→ The model is highly precise; when it predicts a sample as positive, it's usually correct.\n",
    "\n",
    "🔶 Recall: 0.9761\n",
    "\n",
    "→ The model retrieves almost all actual positive samples, indicating very strong sensitivity.\n",
    "\n",
    "🔶 F1 Score: 0.9748\n",
    "\n",
    "→ The harmonic mean of precision and recall is excellent, confirming a good balance between the two.\n",
    "\n",
    "🔶 Class 0 (Majority Class)\n",
    "\n",
    "Precision: 0.98\n",
    "\n",
    "Recall: 1.00\n",
    "\n",
    "F1-score: 0.99\n",
    "\n",
    "→ The model is nearly perfect in identifying the negative class, with just 7 false positives out of 1,729.\n",
    "\n",
    "🔶 Class 1 (Minority Class)\n",
    "\n",
    "Precision: 0.94\n",
    "\n",
    "Recall: 0.75\n",
    "\n",
    "F1-score: 0.84\n",
    "\n",
    "→ Strong precision means most predicted positives are correct, but recall of 0.75 shows it misses about 25% of true positives (38 out of 155). This could still be improved depending on application sensitivity.\n",
    "\n",
    "🔶 Conclusion:\n",
    "\n",
    "The Tuned Ensemble Model demonstrates strong, reliable performance across all key metrics.\n",
    "\n",
    "Class 0 detection is nearly flawless\n",
    "\n",
    "Class 1 detection is also strong, but with slightly lower recall — suggesting room to enhance detection of minority class positives.\n",
    "\n",
    "This model is well-balanced, handles class imbalance reasonably well, and is suitable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46bc910-2b5c-4029-ab55-a556c3acb775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List to store the final evaluation results of the models\n",
    "results = []\n",
    "\n",
    "# Define a function to evaluate a model by calculating and storing various metrics\n",
    "def evaluate_model(name, model, X_train, X_test, y_train, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    # If the model supports probability estimates, get the probability for class 1\n",
    "    y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
    "\n",
    "    # Store the evaluation metrics in the results list\n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'Precision': precision_score(y_test, y_pred, average='weighted'),\n",
    "        'Recall': recall_score(y_test, y_pred, average='weighted'),\n",
    "        'F1 Score': f1_score(y_test, y_pred, average='weighted'),\n",
    "        'ROC AUC': roc_auc_score(y_test, y_proba) if y_proba is not None else 'N/A'\n",
    "    })\n",
    "\n",
    "# Assumption: Only KNN model has been trained so far, and we want to evaluate it\n",
    "evaluate_model(\"Logistic Regression\", best_log_reg_model, x_train, x_test, y_train, y_test)\n",
    "evaluate_model(\"KNN\", best_knn_model, x_train_knn, x_test_knn, y_train_knn, y_test_knn)\n",
    "evaluate_model(\"Complement Naive Bayes\", best_cnb_model, x_train_cnb, x_test_cnb, y_train_cnb, y_test_cnb)\n",
    "evaluate_model(\"Ensemble Model(Logistic Regression + KNN)\", voting_clf, x_train_log_reg_knn, x_test_log_reg_knn, y_train_log_reg_knn, y_test_log_reg_knn)\n",
    "\n",
    "# Convert the results list to a DataFrame for tabular display\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print(\"🔶 Final Evaluation Metrics Table:\")\n",
    "display(results_df.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec06574-3130-4e13-b84b-57b7c59393ca",
   "metadata": {},
   "source": [
    "## <b> Predict for User input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629973c6-fc44-40e3-8d4e-c5d314140814",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_personal_loan(model_name='Logistic Regression'):\n",
    "    \"\"\"\n",
    "    Gets input from user and predicts Personal Loan status using selected model.\n",
    "    \"\"\"\n",
    "    # Get user input as a DataFrame (assumes get_user_input is defined elsewhere)\n",
    "    input_df = get_user_input()\n",
    "    \n",
    "    # Preprocessing and model selection based on user's chosen model\n",
    "    if model_name == 'Logistic Regression':\n",
    "        x_input = scaler_log_reg.transform(input_df) # Apply the logistic regression scaler\n",
    "        model = best_log_reg_model                   # Use the best logistic regression model\n",
    "    elif model_name == 'KNN':\n",
    "        x_input = scaler_knn.transform(input_df)     # Apply the KNN scaler\n",
    "        model = best_knn_model                            # Use the best KNN model\n",
    "    elif model_name == 'Complement Naive Bayes':\n",
    "        x_input = scaler_cnb.transform(input_df)      # Apply the Naive Bayes scaler\n",
    "        model = best_cnb_model                       # Use the best ComplementNB model\n",
    "    elif model_name == 'Ensemble Model(Logistic Regression + KNN)':\n",
    "        x_input = scaler_log_reg.transform(input_df)\n",
    "        model = voting_clf\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model name. Choose from: 'Logistic Regression', 'KNN', 'Complement Naive Bayes'\")\n",
    "\n",
    "    # Make prediction using the selected model\n",
    "    prediction = model.predict(x_input)[0]\n",
    "    # If the model supports probability output, get the probability of class 1 (taking loan)\n",
    "    proba = model.predict_proba(x_input)[0][1] if hasattr(model, \"predict_proba\") else None\n",
    "\n",
    "    print(f\"\\nModel Used: {model_name}\")\n",
    "    print(f\"Prediction: {'Will take loan (1)' if prediction == 1 else 'Will NOT take loan (0)'}\")\n",
    "    if proba is not None:\n",
    "        print(f\"Probability of taking loan: {proba:.2f}\\n\\n\")\n",
    "\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8abfa4-bf80-4a50-8dec-cf532302e3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_input():\n",
    "    \"\"\"\n",
    "    Gets input from user for all required features and returns as DataFrame.\n",
    "    \"\"\"\n",
    "    # List of all features expected by the model\n",
    "    feature_names = [\n",
    "        'ID', 'Age', 'Experience', 'Income', 'ZIP Code', 'Family', 'CCAvg_Annual', 'Education',\n",
    "        'Mortgage', 'Securities Account', 'CD Account', 'Online', 'CreditCard'\n",
    "    ]\n",
    "\n",
    "    user_data = {} # Dictionary to store user inputs\n",
    "    for feature in feature_names:\n",
    "        while True:\n",
    "            try:\n",
    "                if feature == 'CCAvg_Annual':\n",
    "                    value = input(f\"🔸 Enter value for '{feature}' (fraction like 1/2 or integer only): \").strip()\n",
    "                    if '/' in value: # Convert fraction like '1/2' to float\n",
    "                        num, denom = value.split('/')\n",
    "                        value = float(num) / float(denom)\n",
    "                    elif value.isdigit():\n",
    "                        value = float(value)\n",
    "                    else:\n",
    "                        raise ValueError(\"Only fractions or integers are allowed.\")\n",
    "                else:\n",
    "                    value = float(input(f\"🔸 Enter value for '{feature}': \"))\n",
    "                \n",
    "                user_data[feature] = value\n",
    "                break\n",
    "            except ValueError as e:\n",
    "                print(f\"Invalid input: {e}\") # Handle non-numeric inputs\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    input_df = pd.DataFrame([user_data])\n",
    "\n",
    "    # Add lat/lng based on Zip Code\n",
    "    zip_code = int(user_data['ZIP Code'])\n",
    "    if zip_code in zip_df['zip'].values:\n",
    "        lat = zip_df.loc[zip_df['zip'] == zip_code, 'lat'].values[0]\n",
    "        lng = zip_df.loc[zip_df['zip'] == zip_code, 'lng'].values[0]\n",
    "    else:\n",
    "        raise ValueError(\"Zip Code not found in reference data!\")\n",
    "\n",
    "    # Add lat/lng\n",
    "    input_df['lat'] = lat\n",
    "    input_df['lng'] = lng\n",
    "\n",
    "    # Add any engineered feature like CCAvg_Annual if your model uses it\n",
    "    input_df['CCAvg_Annual'] = input_df['CCAvg_Annual'] * 12\n",
    "\n",
    "    # Drop zip code if model doesn't use it\n",
    "    input_df.drop(columns=['ZIP Code', 'ID'], inplace=True)\n",
    "\n",
    "    return input_df\n",
    "\n",
    "predict_personal_loan(model_name='Logistic Regression')\n",
    "predict_personal_loan(model_name='KNN')\n",
    "predict_personal_loan(model_name='Complement Naive Bayes')\n",
    "predict_personal_loan(model_name='Ensemble Model(Logistic Regression + KNN)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dcdf8c-d9cc-4e3c-be9c-baf858037817",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
